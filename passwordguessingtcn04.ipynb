{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\"\"\"\n",
    "ENHANCED PGTCN - Multi-Strategy Password Generation\n",
    "GPU-Optimized for CUDA 12.8 with RTX 4500 Ada\n",
    "Implements 5 key improvements to achieve 20-40% match rate\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing GPU...\n",
      "\n",
      "================================================================================\n",
      "GPU STATUS CHECK\n",
      "================================================================================\n",
      "‚úÖ CUDA Available: Yes\n",
      "üéÆ GPU Device: NVIDIA RTX 4500 Ada Generation\n",
      "üî¢ CUDA Version: 13.0\n",
      "üíæ Total VRAM: 25.25 GB\n",
      "üîß Device Capability: (8, 9)\n",
      "üìä Number of GPUs: 1\n",
      "üéØ Current Device: 0\n",
      "üöÄ TF32 enabled for matrix multiplication\n",
      "üöÄ TF32 enabled for cuDNN\n",
      "üöÄ cuDNN benchmark mode enabled\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: GPU Monitoring Functions\n",
    "# ============================================\n",
    "# GPU MONITORING & VERIFICATION\n",
    "# ============================================\n",
    "\n",
    "def check_gpu_status():\n",
    "    \"\"\"Check and display GPU status\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GPU STATUS CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ CUDA Available: Yes\")\n",
    "        print(f\"üéÆ GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üî¢ CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"üíæ Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"üîß Device Capability: {torch.cuda.get_device_capability(0)}\")\n",
    "        print(f\"üìä Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"üéØ Current Device: {torch.cuda.current_device()}\")\n",
    "        \n",
    "        # Enable optimizations\n",
    "        if hasattr(torch.backends.cuda, 'matmul'):\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            print(f\"üöÄ TF32 enabled for matrix multiplication\")\n",
    "        \n",
    "        if hasattr(torch.backends.cudnn, 'allow_tf32'):\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(f\"üöÄ TF32 enabled for cuDNN\")\n",
    "        \n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(f\"üöÄ cuDNN benchmark mode enabled\")\n",
    "        \n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå CUDA NOT AVAILABLE - Will run on CPU\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def monitor_gpu_memory():\n",
    "    \"\"\"Monitor GPU memory usage during training\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        free = total - reserved\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"GPU MEMORY STATUS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Allocated:  {allocated:6.2f} GB ({allocated/total*100:5.1f}%)\")\n",
    "        print(f\"Reserved:   {reserved:6.2f} GB ({reserved/total*100:5.1f}%)\")\n",
    "        print(f\"Free:       {free:6.2f} GB ({free/total*100:5.1f}%)\")\n",
    "        print(f\"Total:      {total:6.2f} GB\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache to free up memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"‚úì GPU cache cleared\")\n",
    "\n",
    "\n",
    "# Initialize GPU at startup\n",
    "print(\"Initializing GPU...\")\n",
    "GPU_AVAILABLE = check_gpu_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Dataset configurations\n",
    "    DATASETS = {\n",
    "        # '000webhost': {\n",
    "        #     'path': 'datasets/000webhost.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/000webhost/',\n",
    "        #     'description': '000webhost password leak'\n",
    "        # },\n",
    "        # '10-million-password-list-top-1000000': {\n",
    "        #     'path': 'datasets/10-million-password-list-top-1000000.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/10-million-password-list-top-1000000/',\n",
    "        #     'description': '10 million password list top 1M'\n",
    "        # },\n",
    "        # 'Ashley-Madison': {\n",
    "        #     'path': 'datasets/Ashley-Madison.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/Ashley-Madiso/',\n",
    "        #     'description': 'Ashley Madison password leak'\n",
    "        # },\n",
    "        # 'hashkiller-dict': {\n",
    "        #     'path': 'datasets/hashkiller-dict.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/hashkiller-dict/',\n",
    "        #     'description': 'Hashkiller dictionary'\n",
    "        # },\n",
    "        # 'hashmob.net.large.found': {\n",
    "        #     'path': 'datasets/hashmob.net.large.found.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/hashmob.net.large.found/',\n",
    "        #     'description': 'Hashmob large found'\n",
    "        # },\n",
    "        # 'honeynet': {\n",
    "        #     'path': 'datasets/honeynet.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/honeynet/',\n",
    "        #     'description': 'Honeynet password leak'\n",
    "        # },\n",
    "        # 'hotmail': {\n",
    "        #     'path': 'datasets/hotmail.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/hotmail/',\n",
    "        #     'description': 'Hotmail password leak'\n",
    "        # },\n",
    "        'myspace': {\n",
    "            'path': 'datasets/myspace.txt',\n",
    "            'model_dir': 'pgtcn_modelMulti/myspace/',\n",
    "            'description': 'MySpace password leak'}\n",
    "        # },\n",
    "        # 'NordVPN': {\n",
    "        #     'path': 'datasets/NordVPN.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/NordVPN/',\n",
    "        #     'description': 'NordVPN password leak'\n",
    "        # },\n",
    "        # 'phpbb': {\n",
    "        #     'path': 'datasets/phpbb.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/phpbb/',\n",
    "        #     'description': 'phpBB password leak'\n",
    "        # },\n",
    "        # 'rockyou': {\n",
    "        #     'path': 'datasets/rockyou.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/rockyou/',\n",
    "        #     'description': 'RockYou password leak (14M passwords)'\n",
    "        # },\n",
    "        # 'singles.org': {\n",
    "        #     'path': 'datasets/singles.org.txt',\n",
    "        #     'model_dir': 'pgtcn_modelMulti/singles.org/',\n",
    "        #     'description': 'Singles.org password leak'\n",
    "        # }\n",
    "    }\n",
    "    \n",
    "    # Model architecture\n",
    "    EMBEDDING_DIM = 512\n",
    "    TCN_CHANNELS = [512, 512, 512, 512, 512, 512]\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.1\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 3e-4\n",
    "    EPOCHS = 20  # Increased for better convergence\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    GRAD_CLIP = 0.5\n",
    "    USE_AMP = True\n",
    "    USE_FOCAL_LOSS = True\n",
    "    FOCAL_ALPHA = 0.25\n",
    "    FOCAL_GAMMA = 2.0\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    \n",
    "    # Data parameters\n",
    "    SEQ_LEN = 18  # Increased to accommodate SOS and EOS tokens\n",
    "    NUM_WORKERS = 4\n",
    "    PIN_MEMORY = True\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Dynamic\n",
    "    VOCAB_SIZE = None\n",
    "    DATA_PATH = None\n",
    "    MODEL_DIR = None\n",
    "    # Generation\n",
    "    BEAM_WIDTH = 5\n",
    "    LENGTH_PENALTY = 0.8\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_dataset(dataset_name):\n",
    "        if dataset_name not in Config.DATASETS:\n",
    "            raise ValueError(f\"Dataset {dataset_name} not found\")\n",
    "        Config.DATA_PATH = Config.DATASETS[dataset_name]['path']\n",
    "        Config.MODEL_DIR = Config.DATASETS[dataset_name]['model_dir']\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_available_datasets():\n",
    "        return list(Config.DATASETS.keys())\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_config():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Embedding Dim:     {Config.EMBEDDING_DIM}\")\n",
    "        print(f\"TCN Channels:      {Config.TCN_CHANNELS}\")\n",
    "        print(f\"Kernel Size:       {Config.KERNEL_SIZE}\")\n",
    "        print(f\"Dropout:           {Config.DROPOUT}\")\n",
    "        print(f\"Batch Size:        {Config.BATCH_SIZE}\")\n",
    "        print(f\"Learning Rate:     {Config.LEARNING_RATE}\")\n",
    "        print(f\"Epochs:            {Config.EPOCHS}\")\n",
    "        print(f\"Device:            {Config.DEVICE}\")\n",
    "        print(f\"Use AMP:           {Config.USE_AMP}\")\n",
    "        print(f\"Use Focal Loss:    {Config.USE_FOCAL_LOSS}\")\n",
    "        print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Multi-Dataset Utilities\n",
    "# ============================================\n",
    "# MULTI-DATASET UTILITIES\n",
    "# ============================================\n",
    "\n",
    "def load_dataset(dataset_name):\n",
    "    \"\"\"Load and prepare a specific dataset\"\"\"\n",
    "    Config.set_dataset(dataset_name)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Loading {Config.DATASETS[dataset_name]['description']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        with open(Config.DATA_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            passwords = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        filtered = [pwd for pwd in passwords if 4 <= len(pwd) <= 16 and pwd.isascii()]\n",
    "        print(f\"‚úì Loaded {len(passwords):,} passwords\")\n",
    "        print(f\"‚úì Filtered: {len(filtered):,} passwords (4-16 chars, ASCII only)\\n\")\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Dataset file not found at {Config.DATA_PATH}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_dataset_model(model, stoi, itos, dataset_name, epoch=None):\n",
    "    \"\"\"Save model and vocabulary for a specific dataset\"\"\"\n",
    "    model_dir = Config.DATASETS[dataset_name]['model_dir']\n",
    "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    if epoch is not None:\n",
    "        model_path = f\"{model_dir}/pgtcn_epoch_{epoch}.pt\"\n",
    "    else:\n",
    "        model_path = f\"{model_dir}/pgtcn_final.pt\"\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = f\"{model_dir}/vocabulary.json\"\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'stoi': stoi,\n",
    "            'itos': {str(k): v for k, v in itos.items()}\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    # Save config\n",
    "    config_path = f\"{model_dir}/config.json\"\n",
    "    config_data = {\n",
    "        'dataset': dataset_name,\n",
    "        'vocab_size': len(stoi),\n",
    "        'embedding_dim': Config.EMBEDDING_DIM,\n",
    "        'tcn_channels': Config.TCN_CHANNELS,\n",
    "        'kernel_size': Config.KERNEL_SIZE,\n",
    "        'dropout': Config.DROPOUT,\n",
    "        'seq_len': Config.SEQ_LEN\n",
    "    }\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Model saved: {model_path}\")\n",
    "    print(f\"‚úì Vocabulary saved: {vocab_path}\")\n",
    "    print(f\"‚úì Config saved: {config_path}\")\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "\n",
    "def load_dataset_model(dataset_name, vocab_size):\n",
    "    \"\"\"Load a trained model for a specific dataset\"\"\"\n",
    "    model_dir = Config.DATASETS[dataset_name]['model_dir']\n",
    "    model_path = f\"{model_dir}/pgtcn_final.pt\"\n",
    "    vocab_path = f\"{model_dir}/vocabulary.json\"\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab_data = json.load(f)\n",
    "        stoi = vocab_data['stoi']\n",
    "        itos = {int(k): v for k, v in vocab_data['itos'].items()}\n",
    "    \n",
    "    # Create model\n",
    "    model = PGTCN(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=Config.EMBEDDING_DIM,\n",
    "        num_channels=Config.TCN_CHANNELS,\n",
    "        kernel_size=Config.KERNEL_SIZE,\n",
    "        dropout=Config.DROPOUT\n",
    "    ).to(Config.DEVICE)\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(torch.load( enabledmodel_path, map_location=Config.DEVICE))\n",
    "    \n",
    "    print(f\"‚úì Model loaded from: {model_path}\")\n",
    "    \n",
    "    return model, stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Vocabulary\n",
    "# ============================================\n",
    "# VOCABULARY\n",
    "# ============================================\n",
    "\n",
    "def build_vocabulary(passwords):\n",
    "    unique_chars = set()\n",
    "    for pwd in passwords:\n",
    "        unique_chars.update(pwd)\n",
    "    \n",
    "    unique_chars = sorted(unique_chars)\n",
    "    \n",
    "    stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "    \n",
    "    for idx, char in enumerate(unique_chars, start=4):\n",
    "        stoi[char] = idx\n",
    "    \n",
    "    itos = {v: k for k, v in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "\n",
    "def tokenize_password(pwd, stoi, max_len=18):\n",
    "    # Add SOS and EOS tokens\n",
    "    tokens = [stoi.get(\"<SOS>\", 1)] + [stoi.get(c, stoi.get(\"<UNK>\", 3)) for c in pwd] + [stoi.get(\"<EOS>\", 2)]\n",
    "    \n",
    "    # Truncate if necessary (keeping SOS and EOS is important, but we prioritize fitting in max_len)\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len-1] + [stoi.get(\"<EOS>\", 2)]\n",
    "    \n",
    "    # Pad\n",
    "    tokens += [stoi.get(\"<PAD>\", 0)] * (max_len - len(tokens))\n",
    "    return tokens[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset with Augmentation\n",
    "# ============================================\n",
    "# ENHANCED DATASET WITH AUGMENTATION\n",
    "# ============================================\n",
    "\n",
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, passwords, stoi, max_len=16, augment=True):\n",
    "        self.passwords = passwords\n",
    "        self.stoi = stoi\n",
    "        self.max_len = max_len\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.passwords)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pwd = self.passwords[idx]\n",
    "        \n",
    "        # Data augmentation - random case flipping\n",
    "        if self.augment and random.random() < 0.1:\n",
    "            pwd = pwd.swapcase()\n",
    "        \n",
    "        tokens = tokenize_password(pwd, self.stoi, self.max_len)\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: TCN Building Blocks\n",
    "# ============================================\n",
    "# TCN BUILDING BLOCKS\n",
    "# ============================================\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Remove padding from the end of sequence\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"Temporal block with dilated causal convolutions\"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First convolution\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Second convolution\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.chomp1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.chomp2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        # Residual connection (NON-INPLACE!)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        out = out + res  # Non-inplace addition\n",
    "        \n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"Temporal Convolutional Network (TCN)\"\"\"\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers.append(TemporalBlock(\n",
    "                in_channels, out_channels, kernel_size, stride=1,\n",
    "                dilation=dilation_size,\n",
    "                padding=(kernel_size-1) * dilation_size,\n",
    "                dropout=dropout\n",
    "            ))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: PGTCN Model\n",
    "# ============================================\n",
    "# ENHANCED PGTCN MODEL\n",
    "# ============================================\n",
    "\n",
    "class PGTCN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, num_channels=[256]*6, \n",
    "                 kernel_size=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding with better initialization\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "        self.embedding.weight.data[0].zero_()\n",
    "        \n",
    "        # TCN\n",
    "        self.tcn = TemporalConvNet(\n",
    "            num_inputs=embedding_dim,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for stability\n",
    "        self.layer_norm = nn.LayerNorm(num_channels[-1])\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc = nn.Linear(num_channels[-1], vocab_size)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x).transpose(1, 2)\n",
    "        tcn_out = self.tcn(emb)\n",
    "        tcn_out = tcn_out.transpose(1, 2)\n",
    "        tcn_out = self.layer_norm(tcn_out)\n",
    "        logits = self.fc(tcn_out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Focal Loss\n",
    "# ============================================\n",
    "# FOCAL LOSS FOR HARD EXAMPLES\n",
    "# ============================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', ignore_index=self.ignore_index)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Nucleus Sampling\n",
    "# ============================================\n",
    "# NUCLEUS (TOP-P) SAMPLING\n",
    "# ============================================\n",
    "\n",
    "def nucleus_sampling(logits, top_p=0.9, temperature=1.0):\n",
    "    \"\"\"Sample from nucleus (top-p) distribution\"\"\"\n",
    "    logits = logits / temperature\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    # Remove tokens with cumulative probability above threshold\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return next_token.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiversePasswordGenerator:\n",
    "    def __init__(self, model, stoi, itos, device='cuda'):\n",
    "        self.model = model\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.device = device\n",
    "        self.generated_cache = set()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_with_nucleus(self, max_len=16, temperature=1.0, top_p=0.9):\n",
    "        \"\"\"Generate with nucleus sampling for diversity\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        x = torch.tensor([[self.stoi[\"<SOS>\"]]], dtype=torch.long, device=self.device)\n",
    "        password = \"\"\n",
    "        \n",
    "        for _ in range(max_len - 1):\n",
    "            logits = self.model(x)\n",
    "            next_logits = logits[0, -1, :]\n",
    "            \n",
    "            next_token = nucleus_sampling(next_logits, top_p=top_p, temperature=temperature)\n",
    "            \n",
    "            if next_token == self.stoi[\"<EOS>\"]:\n",
    "                break\n",
    "            \n",
    "            char = self.itos.get(next_token, \"\")\n",
    "            if char and char not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:\n",
    "                password += char\n",
    "            \n",
    "            x = torch.cat([x, torch.tensor([[next_token]], dtype=torch.long, device=self.device)], dim=1)\n",
    "        \n",
    "        return password.strip()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_with_beam_search(self, max_len=16, beam_width=5, length_penalty=0.8):\n",
    "        \"\"\"Generate with beam search for higher quality\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize beams: (sequence, score, length)\n",
    "        beams = [([self.stoi[\"<SOS>\"]], 0.0, 1)]\n",
    "        \n",
    "        for _ in range(max_len - 1):\n",
    "            new_beams = []\n",
    "            \n",
    "            for seq, score, length in beams:\n",
    "                if seq[-1] == self.stoi[\"<EOS>\"]:\n",
    "                    new_beams.append((seq, score, length))\n",
    "                    continue\n",
    "                \n",
    "                x = torch.tensor([seq], dtype=torch.long, device=self.device)\n",
    "                logits = self.model(x)\n",
    "                next_logits = logits[0, -1, :]\n",
    "                \n",
    "                # Get top-k candidates\n",
    "                probs = F.softmax(next_logits, dim=-1)\n",
    "                top_probs, top_indices = torch.topk(probs, beam_width)\n",
    "                \n",
    "                for prob, idx in zip(top_probs, top_indices):\n",
    "                    new_seq = seq + [idx.item()]\n",
    "                    new_score = score + torch.log(prob).item()\n",
    "                    # Apply length penalty\n",
    "                    penalized_score = new_score / (length ** length_penalty)\n",
    "                    new_beams.append((new_seq, penalized_score, length + 1))\n",
    "            \n",
    "            # Keep top beam_width beams\n",
    "            new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "            beams = new_beams[:beam_width]\n",
    "        \n",
    "        # Choose the best beam\n",
    "        best_seq, _, _ = max(beams, key=lambda x: x[1])\n",
    "        \n",
    "        # Convert to password\n",
    "        password = \"\"\n",
    "        for token in best_seq[1:]:  # Skip <SOS>\n",
    "            if token == self.stoi[\"<EOS>\"]:\n",
    "                break\n",
    "            char = self.itos.get(token, \"\")\n",
    "            if char and char not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:\n",
    "                password += char\n",
    "        \n",
    "        return password.strip()\n",
    "    \n",
    "    def generate_diverse_batch(self, num_passwords=10000, max_attempts_per_pwd=5, use_beam_search=False):\n",
    "        \"\"\"Generate diverse passwords with deduplication\"\"\"\n",
    "        passwords = []\n",
    "        temperature_range = [0.7, 0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "        top_p_range = [0.85, 0.90, 0.95]\n",
    "        \n",
    "        pbar = tqdm(total=num_passwords, desc=\"Generating diverse passwords\")\n",
    "        \n",
    "        attempts = 0\n",
    "        max_total_attempts = num_passwords * max_attempts_per_pwd\n",
    "        \n",
    "        while len(passwords) < num_passwords and attempts < max_total_attempts:\n",
    "            if use_beam_search:\n",
    "                pwd = self.generate_with_beam_search(max_len=Config.SEQ_LEN, beam_width=Config.BEAM_WIDTH, length_penalty=Config.LENGTH_PENALTY)\n",
    "            else:\n",
    "                # Vary temperature and top_p for diversity\n",
    "                temp = random.choice(temperature_range)\n",
    "                top_p = random.choice(top_p_range)\n",
    "                pwd = self.generate_with_nucleus(max_len=Config.SEQ_LEN, temperature=temp, top_p=top_p)\n",
    "            \n",
    "            # Only add unique passwords\n",
    "            if pwd and pwd not in self.generated_cache and len(pwd) >= 4:\n",
    "                passwords.append(pwd)\n",
    "                self.generated_cache.add(pwd)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            attempts += 1\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        return passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Enhanced Trainer\n",
    "# ============================================\n",
    "# ENHANCED TRAINER WITH GPU OPTIMIZATION\n",
    "# ============================================\n",
    "\n",
    "class PGTCNTrainer:\n",
    "    def __init__(self, model, config, stoi, itos):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.device = config.DEVICE\n",
    "        \n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Cosine annealing with warmup\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            T_0=10,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        self.scaler = torch.amp.GradScaler('cuda') if config.USE_AMP else None\n",
    "        \n",
    "        # Use focal loss if enabled\n",
    "        if config.USE_FOCAL_LOSS:\n",
    "            self.criterion = FocalLoss(\n",
    "                alpha=config.FOCAL_ALPHA,\n",
    "                gamma=config.FOCAL_GAMMA,\n",
    "                ignore_index=stoi[\"<PAD>\"]\n",
    "            )\n",
    "        else:\n",
    "            self.criterion = None\n",
    "        \n",
    "        self.best_loss = float('inf')\n",
    "        self.history = []\n",
    "    \n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=\"Training\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch.to(self.device)\n",
    "            \n",
    "            with torch.amp.autocast('cuda', enabled=self.config.USE_AMP):\n",
    "                logits = self.model(input_ids)\n",
    "                shift_logits = logits[:, :-1, :].contiguous()\n",
    "                shift_labels = input_ids[:, 1:].contiguous()\n",
    "                \n",
    "                if self.criterion:\n",
    "                    loss = self.criterion(\n",
    "                        shift_logits.view(-1, self.config.VOCAB_SIZE),\n",
    "                        shift_labels.view(-1)\n",
    "                    )\n",
    "                else:\n",
    "                    loss = F.cross_entropy(\n",
    "                        shift_logits.view(-1, self.config.VOCAB_SIZE),\n",
    "                        shift_labels.view(-1),\n",
    "                        ignore_index=self.stoi[\"<PAD>\"],\n",
    "                        label_smoothing=self.config.LABEL_SMOOTHING\n",
    "                    )\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                continue\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.scaler:\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRAD_CLIP)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRAD_CLIP)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        return np.mean(epoch_losses) if epoch_losses else float('inf')\n",
    "    \n",
    "    def train(self, dataloader):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training ENHANCED PGTCN for {self.config.EPOCHS} epochs\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for epoch in range(self.config.EPOCHS):\n",
    "            avg_loss = self.train_epoch(dataloader)\n",
    "            \n",
    "            if avg_loss < self.best_loss:\n",
    "                self.best_loss = avg_loss\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), \n",
    "                          f\"{self.config.MODEL_DIR}/pgtcn_best.pt\")\n",
    "            \n",
    "            self.history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'loss': avg_loss,\n",
    "                'lr': self.optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{self.config.EPOCHS} | Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Best: {self.best_loss:.4f} | LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_loss < 0.1:\n",
    "                print(f\"\\n‚úÖ Excellent loss achieved! Stopping early.\\n\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training complete! Best loss: {self.best_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Evaluation\n",
    "# ============================================\n",
    "# EVALUATION\n",
    "# ============================================\n",
    "\n",
    "def evaluate_matching(generated, test_set):\n",
    "    unique_gen = set(generated)\n",
    "    matches = unique_gen & test_set\n",
    "    \n",
    "    # Additional metrics\n",
    "    gen_lengths = [len(p) for p in generated]\n",
    "    test_lengths = [len(p) for p in test_set]\n",
    "    \n",
    "    results = {\n",
    "        'total_generated': len(generated),\n",
    "        'unique_generated': len(unique_gen),\n",
    "        'uniqueness_rate': len(unique_gen) / len(generated) * 100 if generated else 0,\n",
    "        'test_set_size': len(test_set),\n",
    "        'matches': len(matches),\n",
    "        'match_rate': len(matches) / len(test_set) * 100 if test_set else 0,\n",
    "        'matched_passwords': list(matches)[:50],\n",
    "        'avg_gen_length': np.mean(gen_lengths) if gen_lengths else 0,\n",
    "        'avg_test_length': np.mean(test_lengths) if test_lengths else 0,\n",
    "        'sample_generated': generated[:20]\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Save Functions\n",
    "# ============================================\n",
    "# SAVE GENERATED PASSWORDS TO FILE\n",
    "# ============================================\n",
    "\n",
    "def save_passwords_to_file(passwords, output_dir, filename=\"generated_passwords.txt\"):\n",
    "    \"\"\"Save generated passwords to a text file\"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    output_path = Path(output_dir) / filename\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for pwd in passwords:\n",
    "            f.write(f\"{pwd}\\n\")\n",
    "    \n",
    "    return str(output_path)\n",
    "\n",
    "\n",
    "def save_detailed_results(passwords, test_passwords, output_dir):\n",
    "    \"\"\"Save detailed analysis with multiple files\"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Save all generated passwords\n",
    "    all_pwd_path = save_passwords_to_file(passwords, output_dir, \"all_generated_passwords.txt\")\n",
    "    print(f\"‚úì All passwords saved: {all_pwd_path}\")\n",
    "    \n",
    "    # 2. Save unique passwords only\n",
    "    unique_passwords = list(set(passwords))\n",
    "    unique_pwd_path = save_passwords_to_file(unique_passwords, output_dir, \"unique_generated_passwords.txt\")\n",
    "    print(f\"‚úì Unique passwords saved: {unique_pwd_path}\")\n",
    "    \n",
    "    # 3. Save matched passwords\n",
    "    matches = set(passwords) & test_passwords\n",
    "    if matches:\n",
    "        matched_pwd_path = save_passwords_to_file(sorted(matches), output_dir, \"matched_passwords.txt\")\n",
    "        print(f\"‚úì Matched passwords saved: {matched_pwd_path}\")\n",
    "    \n",
    "    # 4. Save passwords by length\n",
    "    length_groups = {}\n",
    "    for pwd in unique_passwords:\n",
    "        length = len(pwd)\n",
    "        if length not in length_groups:\n",
    "            length_groups[length] = []\n",
    "        length_groups[length].append(pwd)\n",
    "    \n",
    "    length_stats_path = Path(output_dir) / \"passwords_by_length.txt\"\n",
    "    with open(length_stats_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"PASSWORD LENGTH DISTRIBUTION\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for length in sorted(length_groups.keys()):\n",
    "            f.write(f\"\\nLength {length}: {len(length_groups[length])} passwords\\n\")\n",
    "            f.write(\"-\"*60 + \"\\n\")\n",
    "            for pwd in sorted(length_groups[length])[:20]:\n",
    "                f.write(f\"{pwd}\\n\")\n",
    "            if len(length_groups[length]) > 20:\n",
    "                f.write(f\"... and {len(length_groups[length]) - 20} more\\n\")\n",
    "    \n",
    "    print(f\"‚úì Length analysis saved: {length_stats_path}\")\n",
    "    \n",
    "    # 5. Save statistics summary\n",
    "    stats_path = Path(output_dir) / \"generation_statistics.txt\"\n",
    "    with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"PASSWORD GENERATION STATISTICS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Total Generated:       {len(passwords):,}\\n\")\n",
    "        f.write(f\"Unique Passwords:      {len(unique_passwords):,}\\n\")\n",
    "        f.write(f\"Uniqueness Rate:       {len(unique_passwords)/len(passwords)*100:.2f}%\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Test Set Size:         {len(test_passwords):,}\\n\")\n",
    "        f.write(f\"Matched Passwords:     {len(matches):,}\\n\")\n",
    "        f.write(f\"Match Rate:            {len(matches)/len(test_passwords)*100:.2f}%\\n\\n\")\n",
    "        \n",
    "        # Length statistics\n",
    "        lengths = [len(p) for p in unique_passwords]\n",
    "        f.write(f\"Average Length:        {np.mean(lengths):.2f}\\n\")\n",
    "        f.write(f\"Min Length:            {min(lengths)}\\n\")\n",
    "        f.write(f\"Max Length:            {max(lengths)}\\n\")\n",
    "        f.write(f\"Median Length:         {np.median(lengths):.0f}\\n\\n\")\n",
    "        \n",
    "        # Character statistics\n",
    "        f.write(\"CHARACTER USAGE:\\n\")\n",
    "        all_chars = ''.join(unique_passwords)\n",
    "        char_counter = Counter(all_chars)\n",
    "        f.write(f\"Total Characters:      {len(all_chars):,}\\n\")\n",
    "        f.write(f\"Unique Characters:     {len(char_counter)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Top 20 Most Common Characters:\\n\")\n",
    "        for char, count in char_counter.most_common(20):\n",
    "            f.write(f\"  '{char}': {count:,} ({count/len(all_chars)*100:.2f}%)\\n\")\n",
    "    \n",
    "    print(f\"‚úì Statistics saved: {stats_path}\")\n",
    "    \n",
    "    return {\n",
    "        'all_passwords': all_pwd_path,\n",
    "        'unique_passwords': unique_pwd_path,\n",
    "        'matched_passwords': matched_pwd_path if matches else None,\n",
    "        'length_analysis': str(length_stats_path),\n",
    "        'statistics': str(stats_path)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Main Training Functions (Part 1)\n",
    "# ============================================\n",
    "# MAIN TRAINING FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def train_single_dataset(dataset_name, num_passwords_to_generate=500000):\n",
    "    \"\"\"Train model on a single dataset with GPU optimization\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ENHANCED PGTCN - Training on {dataset_name.upper()}\")\n",
    "    print(f\"Target: Generate {num_passwords_to_generate:,} passwords\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Check GPU status\n",
    "    if GPU_AVAILABLE:\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    # Load dataset\n",
    "    filtered = load_dataset(dataset_name)\n",
    "    if filtered is None:\n",
    "        print(f\"‚ùå Skipping {dataset_name} due to loading error\\n\")\n",
    "        return None\n",
    "    \n",
    "    # Build vocabulary\n",
    "    stoi, itos = build_vocabulary(filtered)\n",
    "    Config.VOCAB_SIZE = len(stoi)\n",
    "    print(f\"‚úì Vocabulary size: {Config.VOCAB_SIZE}\\n\")\n",
    "    \n",
    "    # Split train/test (80/20)\n",
    "    random.shuffle(filtered)\n",
    "    split_idx = int(0.8 * len(filtered))\n",
    "    train_passwords = filtered[:split_idx]\n",
    "    test_passwords = set(filtered[split_idx:])\n",
    "    \n",
    "    print(f\"‚úì Train set: {len(train_passwords):,}\")\n",
    "    print(f\"‚úì Test set: {len(test_passwords):,}\\n\")\n",
    "    \n",
    "    # Dataset with augmentation\n",
    "    train_dataset = PasswordDataset(train_passwords, stoi, Config.SEQ_LEN, augment=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=Config.PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    model = PGTCN(\n",
    "        vocab_size=Config.VOCAB_SIZE,\n",
    "        embedding_dim=Config.EMBEDDING_DIM,\n",
    "        num_channels=Config.TCN_CHANNELS,\n",
    "        kernel_size=Config.KERNEL_SIZE,\n",
    "        dropout=Config.DROPOUT\n",
    "    ).to(Config.DEVICE)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"‚úì Model parameters: {num_params:,}\")\n",
    "    print(f\"‚úì Model on device: {next(model.parameters()).device}\\n\")\n",
    "    \n",
    "    # Show GPU memory after model loading\n",
    "    if GPU_AVAILABLE:\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    # Print configuration\n",
    "    Config.print_config()\n",
    "    \n",
    "    # Train\n",
    "    trainer = PGTCNTrainer(model, Config, stoi, itos)\n",
    "    trainer.train(train_loader)\n",
    "    \n",
    "    # Clear GPU cache before generation\n",
    "    if GPU_AVAILABLE:\n",
    "        clear_gpu_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = f\"{Config.MODEL_DIR}/pgtcn_best.pt\"\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=Config.DEVICE))\n",
    "    \n",
    "    # Save final model\n",
    "    save_dataset_model(model, stoi, itos, dataset_name)\n",
    "    \n",
    "    # Generate passwords\n",
    "    print(\"=\"*80)\n",
    "    print(f\"GENERATING {num_passwords_to_generate:,} DIVERSE PASSWORDS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    generator = DiversePasswordGenerator(model, stoi, itos, Config.DEVICE)\n",
    "    generated = generator.generate_diverse_batch(num_passwords=num_passwords_to_generate)\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully generated {len(generated):,} passwords\\n\")\n",
    "    \n",
    "    # Show final GPU memory usage\n",
    "    if GPU_AVAILABLE:\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    results = evaluate_matching(generated, test_passwords)\n",
    "    \n",
    "    print(f\"Generated Passwords:  {results['total_generated']:,}\")\n",
    "    print(f\"Unique Generated:     {results['unique_generated']:,} ({results['uniqueness_rate']:.1f}%)\")\n",
    "    print(f\"Test Set Size:        {results['test_set_size']:,}\")\n",
    "    print(f\"Exact Matches:        {results['matches']:,}\")\n",
    "    print(f\"üéØ MATCH RATE:        {results['match_rate']:.2f}%\")\n",
    "    print(f\"Avg Gen Length:       {results['avg_gen_length']:.1f}\")\n",
    "    print(f\"Avg Test Length:      {results['avg_test_length']:.1f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAVING RESULTS TO FILES\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    json_path = f\"{Config.MODEL_DIR}/results.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"‚úì JSON results saved: {json_path}\\n\")\n",
    "    \n",
    "    saved_files = save_detailed_results(generated, test_passwords, Config.MODEL_DIR)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FILES SAVED:\")\n",
    "    print(\"=\"*80)\n",
    "    for file_type, file_path in saved_files.items():\n",
    "        if file_path:\n",
    "            print(f\"  ‚Ä¢ {file_type}: {file_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"‚úÖ {dataset_name.upper()} Complete!\")\n",
    "    print(f\"   Generated: {len(generated):,} passwords\")\n",
    "    print(f\"   Match Rate: {results['match_rate']:.2f}%\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    if GPU_AVAILABLE:\n",
    "        clear_gpu_cache()\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'results': results,\n",
    "        'files': saved_files,\n",
    "        'model_dir': Config.MODEL_DIR,\n",
    "        'num_generated': len(generated)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Main Training Functions (Part 2)\n",
    "def test_on_dataset(model, stoi, itos, test_dataset_name, source_dataset_name, num_passwords=100000):\n",
    "    \"\"\"Test a trained model on a different dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"CROSS-DATASET TESTING - Generating {num_passwords:,} passwords\")\n",
    "    print(f\"Model: {source_dataset_name.upper()} ‚Üí Test: {test_dataset_name.upper()}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_passwords = load_dataset(test_dataset_name)\n",
    "    if test_passwords is None:\n",
    "        return None\n",
    "    \n",
    "    test_set = set(test_passwords)\n",
    "    \n",
    "    # Generate passwords\n",
    "    generator = DiversePasswordGenerator(model, stoi, itos, Config.DEVICE)\n",
    "    generated = generator.generate_diverse_batch(num_passwords=num_passwords)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_matching(generated, test_set)\n",
    "    \n",
    "    print(f\"Generated Passwords:  {results['total_generated']:,}\")\n",
    "    print(f\"Unique Generated:     {results['unique_generated']:,} ({results['uniqueness_rate']:.1f}%)\")\n",
    "    print(f\"Test Set Size:        {results['test_set_size']:,}\")\n",
    "    print(f\"Exact Matches:        {results['matches']:,}\")\n",
    "    print(f\"üéØ MATCH RATE:        {results['match_rate']:.2f}%\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Save cross-testing results\n",
    "    output_dir = f\"{Config.DATASETS[source_dataset_name]['model_dir']}/cross_test_{test_dataset_name}/\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    json_path = f\"{output_dir}/results.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    saved_files = save_detailed_results(generated, test_set, output_dir)\n",
    "    \n",
    "    return {\n",
    "        'source_dataset': source_dataset_name,\n",
    "        'test_dataset': test_dataset_name,\n",
    "        'results': results,\n",
    "        'files': saved_files\n",
    "    }\n",
    "\n",
    "\n",
    "def main(num_passwords_per_dataset=100000):\n",
    "    \"\"\"Main function to train and generate passwords on multiple datasets\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENHANCED PGTCN - Multi-Dataset Training & Password Generation\")\n",
    "    print(f\"GPU-Optimized for CUDA 12.8 with RTX 4500 Ada\")\n",
    "    print(f\"Generating {num_passwords_per_dataset:,} passwords per dataset\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Show available datasets\n",
    "    available = Config.get_available_datasets()\n",
    "    print(\"Available datasets:\")\n",
    "    for i, dataset in enumerate(available, 1):\n",
    "        desc = Config.DATASETS[dataset]['description']\n",
    "        print(f\"  {i}. {dataset} - {desc}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"TRAINING ON ALL DATASETS - {num_passwords_per_dataset:,} passwords each\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    total_passwords_generated = 0\n",
    "    \n",
    "    for dataset_name in available:\n",
    "        try:\n",
    "            print(f\"\\n{'#'*80}\")\n",
    "            print(f\"# PROCESSING: {dataset_name.upper()}\")\n",
    "            print(f\"{'#'*80}\\n\")\n",
    "            \n",
    "            result = train_single_dataset(dataset_name, num_passwords_per_dataset)\n",
    "            \n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "                total_passwords_generated += result['num_generated']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"‚ùå Error training on {dataset_name}: {str(e)}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING & GENERATION SUMMARY\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"‚ùå No datasets were successfully processed!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"{'Dataset':<15} {'Generated':>12} {'Unique':>12} {'Match Rate':>12} {'Matches':>10}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for result in all_results:\n",
    "        dataset = result['dataset']\n",
    "        num_gen = result['results']['total_generated']\n",
    "        num_unique = result['results']['unique_generated']\n",
    "        match_rate = result['results']['match_rate']\n",
    "        matches = result['results']['matches']\n",
    "        \n",
    "        print(f\"{dataset.upper():<15} \"\n",
    "              f\"{num_gen:>11,} \"\n",
    "              f\"{num_unique:>11,} \"\n",
    "              f\"{match_rate:>11.2f}% \"\n",
    "              f\"{matches:>10,}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTOTAL PASSWORDS GENERATED: {total_passwords_generated:,}\")\n",
    "    print(f\"DATASETS PROCESSED: {len(all_results)}/{len(available)}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Save overall summary\n",
    "    summary_path = \"./pgtcn_modelMulti/training_summary.json\"\n",
    "    Path(\"./pgtcn_modelMulti/\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    summary_data = {\n",
    "        'total_passwords_generated': total_passwords_generated,\n",
    "        'datasets_processed': len(all_results),\n",
    "        'datasets_failed': len(available) - len(all_results),\n",
    "        'passwords_per_dataset': num_passwords_per_dataset,\n",
    "        'results': [\n",
    "            {\n",
    "                'dataset': r['dataset'],\n",
    "                'generated': r['results']['total_generated'],\n",
    "                'unique': r['results']['unique_generated'],\n",
    "                'match_rate': r['results']['match_rate'],\n",
    "                'matches': r['results']['matches'],\n",
    "                'model_dir': r['model_dir']\n",
    "            }\n",
    "            for r in all_results\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úì Overall summary saved: {summary_path}\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úÖ ALL DATASETS PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def train_specific_dataset(dataset_name, num_passwords=500000):\n",
    "    \"\"\"Quick function to train on a specific dataset\"\"\"\n",
    "    if dataset_name not in Config.get_available_datasets():\n",
    "        print(f\"‚ùå Dataset '{dataset_name}' not found!\")\n",
    "        print(f\"Available: {Config.get_available_datasets()}\")\n",
    "        return None\n",
    "    \n",
    "    return train_single_dataset(dataset_name, num_passwords)\n",
    "\n",
    "\n",
    "def train_specific_datasets(dataset_list, num_passwords=500000):\n",
    "    \"\"\"Train on multiple specific datasets\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TRAINING ON SPECIFIC DATASETS - {num_passwords:,} passwords each\")\n",
    "    print(f\"Datasets: {', '.join(dataset_list)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    results = []\n",
    "    total_passwords = 0\n",
    "    \n",
    "    for dataset in dataset_list:\n",
    "        if dataset not in Config.get_available_datasets():\n",
    "            print(f\"‚ö†Ô∏è  Skipping unknown dataset: {dataset}\")\n",
    "            continue\n",
    "        \n",
    "        result = train_specific_dataset(dataset, num_passwords)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            total_passwords += result['num_generated']\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SPECIFIC DATASETS SUMMARY\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for result in results:\n",
    "        dataset = result['dataset']\n",
    "        match_rate = result['results']['match_rate']\n",
    "        num_gen = result['num_generated']\n",
    "        print(f\"{dataset.upper():15} ‚Üí Generated: {num_gen:,} | Match Rate: {match_rate:6.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTotal passwords generated: {total_passwords:,}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENHANCED PGTCN - Multi-Dataset Training & Password Generation\n",
      "GPU-Optimized for CUDA 12.8 with RTX 4500 Ada\n",
      "Generating 1,000,000 passwords per dataset\n",
      "================================================================================\n",
      "\n",
      "Available datasets:\n",
      "  1. myspace - MySpace password leak\n",
      "\n",
      "================================================================================\n",
      "TRAINING ON ALL DATASETS - 1,000,000 passwords each\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# PROCESSING: MYSPACE\n",
      "################################################################################\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ENHANCED PGTCN - Training on MYSPACE\n",
      "Target: Generate 1,000,000 passwords\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "GPU MEMORY STATUS\n",
      "============================================================\n",
      "Allocated:    0.00 GB (  0.0%)\n",
      "Reserved:     0.00 GB (  0.0%)\n",
      "Free:        25.25 GB (100.0%)\n",
      "Total:       25.25 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Loading MySpace password leak\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded 37,126 passwords\n",
      "‚úì Filtered: 36,829 passwords (4-16 chars, ASCII only)\n",
      "\n",
      "‚úì Vocabulary size: 95\n",
      "\n",
      "‚úì Train set: 29,463\n",
      "‚úì Test set: 7,366\n",
      "\n",
      "‚úì Model parameters: 9,541,727\n",
      "‚úì Model on device: cuda:0\n",
      "\n",
      "\n",
      "============================================================\n",
      "GPU MEMORY STATUS\n",
      "============================================================\n",
      "Allocated:    0.04 GB (  0.2%)\n",
      "Reserved:     0.04 GB (  0.2%)\n",
      "Free:        25.20 GB ( 99.8%)\n",
      "Total:       25.25 GB\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CONFIGURATION\n",
      "============================================================\n",
      "Embedding Dim:     512\n",
      "TCN Channels:      [512, 512, 512, 512, 512, 512]\n",
      "Kernel Size:       3\n",
      "Dropout:           0.1\n",
      "Batch Size:        64\n",
      "Learning Rate:     0.0003\n",
      "Epochs:            20\n",
      "Device:            cuda\n",
      "Use AMP:           True\n",
      "Use Focal Loss:    True\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training ENHANCED PGTCN for 20 epochs\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:06<00:00, 76.37it/s, loss=0.3213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Loss: 0.3394 | Best: 0.3394 | LR: 3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.54it/s, loss=0.3158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Loss: 0.3081 | Best: 0.3081 | LR: 2.93e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.71it/s, loss=0.2878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Loss: 0.2956 | Best: 0.2956 | LR: 2.71e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.56it/s, loss=0.2990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Loss: 0.2869 | Best: 0.2869 | LR: 2.38e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.63it/s, loss=0.2841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Loss: 0.2790 | Best: 0.2790 | LR: 1.97e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.97it/s, loss=0.2711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Loss: 0.2724 | Best: 0.2724 | LR: 1.50e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.57it/s, loss=0.2754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Loss: 0.2662 | Best: 0.2662 | LR: 1.04e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.22it/s, loss=0.2760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Loss: 0.2609 | Best: 0.2609 | LR: 6.26e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.63it/s, loss=0.2377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Loss: 0.2566 | Best: 0.2566 | LR: 2.96e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.48it/s, loss=0.2996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Loss: 0.2541 | Best: 0.2541 | LR: 8.32e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.96it/s, loss=0.2670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Loss: 0.2678 | Best: 0.2541 | LR: 3.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 87.32it/s, loss=0.2532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Loss: 0.2630 | Best: 0.2541 | LR: 2.98e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.85it/s, loss=0.2619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Loss: 0.2583 | Best: 0.2541 | LR: 2.93e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.06it/s, loss=0.2499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Loss: 0.2534 | Best: 0.2534 | LR: 2.84e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 87.19it/s, loss=0.2195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Loss: 0.2484 | Best: 0.2484 | LR: 2.71e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 87.23it/s, loss=0.2469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Loss: 0.2430 | Best: 0.2430 | LR: 2.56e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 87.16it/s, loss=0.2339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Loss: 0.2381 | Best: 0.2381 | LR: 2.38e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.68it/s, loss=0.2215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Loss: 0.2329 | Best: 0.2329 | LR: 2.18e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 87.14it/s, loss=0.2366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Loss: 0.2279 | Best: 0.2279 | LR: 1.97e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:05<00:00, 86.86it/s, loss=0.2160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Loss: 0.2233 | Best: 0.2233 | LR: 1.74e-04\n",
      "\n",
      "‚úÖ Training complete! Best loss: 0.2233\n",
      "\n",
      "‚úì GPU cache cleared\n",
      "‚úì Model saved: pgtcn_modelMulti/myspace//pgtcn_final.pt\n",
      "‚úì Vocabulary saved: pgtcn_modelMulti/myspace//vocabulary.json\n",
      "‚úì Config saved: pgtcn_modelMulti/myspace//config.json\n",
      "================================================================================\n",
      "GENERATING 1,000,000 DIVERSE PASSWORDS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating diverse passwords:   0%|   | 2751/1000000 [01:57<11:28:37, 24.14it/s]"
     ]
    }
   ],
   "source": [
    "# Cell 17: Run Training\n",
    "# ============================================\n",
    "# RUN TRAINING - Choose your option\n",
    "# ============================================\n",
    "\n",
    "# OPTION 1: Train ALL datasets with 500K passwords each\n",
    "results = main(num_passwords_per_dataset=1000000)\n",
    "\n",
    "# OPTION 2: Train SPECIFIC datasets\n",
    "# datasets_to_train = ['myspace', 'rockyou']\n",
    "# results = train_specific_datasets(datasets_to_train, num_passwords=500000)\n",
    "\n",
    "# OPTION 3: Train SINGLE dataset\n",
    "# result = train_specific_dataset('myspace', num_passwords=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17.5: Enhanced Training with Beam Search\n",
    "# ============================================\n",
    "# ENHANCED TRAINING OPTIONS\n",
    "# ============================================\n",
    "\n",
    "def train_single_dataset_enhanced(dataset_name, num_passwords_to_generate=500000, use_beam_search=False):\n",
    "    \"\"\"Enhanced training with beam search option\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ENHANCED PGTCN - Training on {dataset_name.upper()}\")\n",
    "    print(f\"Target: Generate {num_passwords_to_generate:,} passwords\")\n",
    "    print(f\"Beam Search: {use_beam_search}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Check GPU status\n",
    "    if GPU_AVAILABLE:\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    # Load dataset\n",
    "    filtered = load_dataset(dataset_name)\n",
    "    if filtered is None:\n",
    "        print(f\"‚ùå Skipping {dataset_name} due to loading error\\n\")\n",
    "        return None\n",
    "    \n",
    "    # Build vocabulary\n",
    "    stoi, itos = build_vocabulary(filtered)\n",
    "    Config.VOCAB_SIZE = len(stoi)\n",
    "    print(f\"‚úì Vocabulary size: {Config.VOCAB_SIZE}\\n\")\n",
    "    \n",
    "    # Split train/test (80/20)\n",
    "    random.shuffle(filtered)\n",
    "    split_idx = int(0.8 * len(filtered))\n",
    "    train_passwords = filtered[:split_idx]\n",
    "    test_passwords = set(filtered[split_idx:])\n",
    "    \n",
    "    print(f\"‚úì Train set: {len(train_passwords):,}\")\n",
    "    print(f\"‚úì Test set: {len(test_passwords):,}\\n\")\n",
    "    \n",
    "    # Dataset with augmentation\n",
    "    train_dataset = PasswordDataset(train_passwords, stoi, Config.SEQ_LEN, augment=True)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        pin_memory=Config.PIN_MEMORY,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    # Build larger model\n",
    "    model = PGTCN(\n",
    "        vocab_size=Config.VOCAB_SIZE,\n",
    "        embedding_dim=Config.EMBEDDING_DIM,\n",
    "        num_channels=Config.TCN_CHANNELS,\n",
    "        kernel_size=Config.KERNEL_SIZE,\n",
    "        dropout=Config.DROPOUT\n",
    "    ).to(Config.DEVICE)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"‚úì Model parameters: {num_params:,}\")\n",
    "    print(f\"‚úì Model on device: {next(model.parameters()).device}\\n\")\n",
    "    \n",
    "    # Show GPU memory after model loading\n",
    "    if GPU_AVAILABLE:\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    # Print configuration\n",
    "    Config.print_config()\n",
    "    \n",
    "    # Train\n",
    "    trainer = PGTCNTrainer(model, Config, stoi, itos)\n",
    "    trainer.train(train_loader)\n",
    "    \n",
    "    # Clear GPU cache before generation\n",
    "    if GPU_AVAILABLE:\n",
    "        clear_gpu_cache()\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = f\"{Config.MODEL_DIR}/pgtcn_best.pt\"\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=Config.DEVICE))\n",
    "    \n",
    "    # Save final model\n",
    "    save_dataset_model(model, stoi, itos, dataset_name)\n",
    "    \n",
    "    # Generate passwords with beam search if enabled\n",
    "    print(\"=\"*80)\n",
    "    print(f\"GENERATING {num_passwords_to_generate:,} PASSWORDS\")\n",
    "    print(f\"Using {'Beam Search' if use_beam_search else 'Nucleus Sampling'}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    generator = DiversePasswordGenerator(model, stoi, itos, Config.DEVICE)\n",
    "    generated = generator.generate_diverse_batch(num_passwords=num_passwords_to_generate, use_beam_search=use_beam_search)\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully generated {len(generated):,} passwords\\n\")\n",
    "    \n",
    "    # Show final GPU memory usage\n",
    "    if GPU_AVAILABLE:\n",
    "        monitor_gpu_memory()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    results = evaluate_matching(generated, test_passwords)\n",
    "    \n",
    "    print(f\"Generated Passwords:  {results['total_generated']:,}\")\n",
    "    print(f\"Unique Generated:     {results['unique_generated']:,} ({results['uniqueness_rate']:.1f}%)\")\n",
    "    print(f\"Test Set Size:        {results['test_set_size']:,}\")\n",
    "    print(f\"Exact Matches:        {results['matches']:,}\")\n",
    "    print(f\"üéØ MATCH RATE:        {results['match_rate']:.2f}%\")\n",
    "    print(f\"Avg Gen Length:       {results['avg_gen_length']:.1f}\")\n",
    "    print(f\"Avg Test Length:      {results['avg_test_length']:.1f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAVING RESULTS TO FILES\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    json_path = f\"{Config.MODEL_DIR}/results_enhanced.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"‚úì JSON results saved: {json_path}\\n\")\n",
    "    \n",
    "    saved_files = save_detailed_results(generated, test_passwords, Config.MODEL_DIR)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FILES SAVED:\")\n",
    "    print(\"=\"*80)\n",
    "    for file_type, file_path in saved_files.items():\n",
    "        if file_path:\n",
    "            print(f\"  ‚Ä¢ {file_type}: {file_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"‚úÖ {dataset_name.upper()} Enhanced Complete!\")\n",
    "    print(f\"   Generated: {len(generated):,} passwords\")\n",
    "    print(f\"   Match Rate: {results['match_rate']:.2f}%\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    if GPU_AVAILABLE:\n",
    "        clear_gpu_cache()\n",
    "    \n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'results': results,\n",
    "        'files': saved_files,\n",
    "        'model_dir': Config.MODEL_DIR,\n",
    "        'num_generated': len(generated),\n",
    "        'beam_search': use_beam_search\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Run Enhanced Training\n",
    "# ============================================\n",
    "# RUN ENHANCED TRAINING FOR ROCKYOU‚úÖ CUDA Available: Yes\n",
    "# ============================================\n",
    "\n",
    "# Train RockYou with enhanced model and beam search\n",
    "result_enhanced = train_single_dataset_enhanced('hashmob.net.large.found', num_passwords_to_generate=1000000, use_beam_search=True)\n",
    "\n",
    "# Compare with nucleus sampling (also using beam search for higher match rate)\n",
    "result_nucleus = train_single_dataset_enhanced('hashmob.net.large.found', num_passwords_to_generate=1000000, use_beam_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    datasets = ['myspace', 'rockyou', 'honeynet']\n",
    "    results = []\n",
    "    for dataset in datasets:\n",
    "        result = train_specific_dataset(dataset)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8691921,
     "sourceId": 13677057,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8690783,
     "sourceId": 13668569,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
