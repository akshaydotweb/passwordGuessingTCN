{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWfahTohjlqm",
        "outputId": "bc69540e-9fbc-4fc7-c732-17cb964fdc97"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt1R4hH5j8Ph"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ijnEWPzzj--B"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'deprecated' from 'typing_extensions' (/Users/lapac/Documents/PES/Capstone Project/.venv/lib/python3.13/site-packages/typing_extensions.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mChomp1d\u001b[39;00m(nn.Module):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PES/Capstone Project/.venv/lib/python3.13/site-packages/torch/__init__.py:52\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_running_with_deploy\u001b[39m() -> builtins.bool:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sys.modules.get(\u001b[33m\"\u001b[39m\u001b[33mtorch._meta_registrations\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     53\u001b[39m     _functionalize_sync \u001b[38;5;28;01mas\u001b[39;00m _sync,\n\u001b[32m     54\u001b[39m     _import_dotted_name,\n\u001b[32m     55\u001b[39m     classproperty,\n\u001b[32m     56\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     58\u001b[39m     get_file_path,\n\u001b[32m     59\u001b[39m     prepare_multiprocessing_environment,\n\u001b[32m     60\u001b[39m     USE_GLOBAL_DEPS,\n\u001b[32m     61\u001b[39m     USE_RTLD_GLOBAL_WITH_LIBTORCH,\n\u001b[32m     62\u001b[39m )\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PES/Capstone Project/.venv/lib/python3.13/site-packages/torch/_utils.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, DefaultDict, Generic, List, Optional, TYPE_CHECKING\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated, ParamSpec\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_type\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, non_blocking=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'deprecated' from 'typing_extensions' (/Users/lapac/Documents/PES/Capstone Project/.venv/lib/python3.13/site-packages/typing_extensions.py)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size]\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, stride, dilation, padding, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size,\n",
        "                               stride=stride, padding=padding, dilation=dilation)\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class PGTCN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size=128, num_channels=[128]*3, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        layers = []\n",
        "        for i, out_ch in enumerate(num_channels):\n",
        "            dilation = 2 ** i\n",
        "            in_ch = emb_size if i == 0 else num_channels[i-1]\n",
        "            layers.append(\n",
        "                TemporalBlock(in_ch, out_ch, kernel_size, stride=1,\n",
        "                              dilation=dilation, padding=(kernel_size-1)*dilation)\n",
        "            )\n",
        "        self.tcn = nn.Sequential(*layers)\n",
        "        self.fc = nn.Linear(num_channels[-1], vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x).transpose(1, 2)\n",
        "        tcn_out = self.tcn(emb).transpose(1, 2)\n",
        "        return self.fc(tcn_out)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uvDkopDnQ3E",
        "outputId": "af2db3ac-f8fd-4989-edc6-44db765ee09a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PGTCN(\n",
              "  (embedding): Embedding(104, 128)\n",
              "  (tcn): Sequential(\n",
              "    (0): TemporalBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
              "      (chomp1): Chomp1d()\n",
              "      (relu1): ReLU()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
              "      (chomp2): Chomp1d()\n",
              "      (relu2): ReLU()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      (net): Sequential(\n",
              "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
              "        (1): Chomp1d()\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,))\n",
              "        (5): Chomp1d()\n",
              "        (6): ReLU()\n",
              "        (7): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (1): TemporalBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
              "      (chomp1): Chomp1d()\n",
              "      (relu1): ReLU()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
              "      (chomp2): Chomp1d()\n",
              "      (relu2): ReLU()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      (net): Sequential(\n",
              "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
              "        (1): Chomp1d()\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
              "        (5): Chomp1d()\n",
              "        (6): ReLU()\n",
              "        (7): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "    (2): TemporalBlock(\n",
              "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
              "      (chomp1): Chomp1d()\n",
              "      (relu1): ReLU()\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
              "      (chomp2): Chomp1d()\n",
              "      (relu2): ReLU()\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      (net): Sequential(\n",
              "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
              "        (1): Chomp1d()\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0.1, inplace=False)\n",
              "        (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
              "        (5): Chomp1d()\n",
              "        (6): ReLU()\n",
              "        (7): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=128, out_features=104, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "path = \"/content/drive/MyDrive/pgtcn_model/\"\n",
        "stoi = pickle.load(open(path + \"stoi.pkl\", \"rb\"))\n",
        "itos = pickle.load(open(path + \"itos.pkl\", \"rb\"))\n",
        "\n",
        "vocab_size = len(stoi)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = PGTCN(vocab_size).to(device)\n",
        "\n",
        "state_dict = torch.load(path + \"pgtcn_model.pt\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z_un0Henqk_"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "def generate_password(model, stoi, itos, max_len=20, temperature=1.0, start_char=\"<SOS>\"):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    if start_char in stoi:\n",
        "        x = torch.tensor([[stoi[start_char]]], dtype=torch.long, device=device)\n",
        "    else:\n",
        "        # fallback: start with a random character\n",
        "        x = torch.tensor([[random.randint(0, len(stoi)-1)]], dtype=torch.long, device=device)\n",
        "\n",
        "    pwd = \"\"\n",
        "    for _ in range(max_len):\n",
        "        logits = model(x)[:, -1, :]              # logits at last step\n",
        "        probs = F.softmax(logits / temperature, dim=-1)\n",
        "        idx = sample_from_logits(logits[-1], temperature=0.7, top_k=20)\n",
        "\n",
        "        char = itos[idx]\n",
        "\n",
        "        if char == \"<EOS>\":  # stop if EOS token exists\n",
        "            break\n",
        "        pwd += char\n",
        "\n",
        "        x = torch.cat([x, torch.tensor([[idx]], device=device)], dim=1)\n",
        "\n",
        "    return pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "hqYFHuvvog5p",
        "outputId": "322b0bbc-7556-43ad-8ce3-2b1c9459573a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sample_from_logits' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-710403893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_password\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generated.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpwd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1555201819.py\u001b[0m in \u001b[0;36mgenerate_password\u001b[0;34m(model, stoi, itos, max_len, temperature, start_char)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m              \u001b[0;31m# logits at last step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_from_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample_from_logits' is not defined"
          ]
        }
      ],
      "source": [
        "generated = [generate_password(model, stoi, itos, max_len=20) for _ in range(100000)]\n",
        "\n",
        "with open(\"generated.txt\", \"w\") as f:\n",
        "    for pwd in generated:\n",
        "        f.write(pwd + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UfV68nV0dtD"
      },
      "outputs": [],
      "source": [
        "# Load your dataset\n",
        "with open(\"/content/drive/MyDrive/pgtcn_model/myspace.txt\") as f:\n",
        "    data = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "random.shuffle(data)\n",
        "split = int(0.8 * len(data))   # 80/20 split\n",
        "test_set = set(data[split:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkWWR52m1onY"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def sample_from_logits(logits, temperature=0.7, top_k=20):\n",
        "    logits = logits / temperature\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    top_probs, top_idx = torch.topk(probs, k=top_k)\n",
        "    idx = torch.multinomial(top_probs, 1).item()\n",
        "    return top_idx[idx].item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1AZ2DnI19G4",
        "outputId": "6f35b626-9a6f-4ab2-cc7e-8c3110f706f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generated 10240 / 1000000\n",
            "generated 20480 / 1000000\n",
            "generated 30720 / 1000000\n",
            "generated 40960 / 1000000\n",
            "generated 51200 / 1000000\n",
            "generated 61440 / 1000000\n",
            "generated 71680 / 1000000\n",
            "generated 81920 / 1000000\n",
            "generated 92160 / 1000000\n",
            "generated 102400 / 1000000\n",
            "generated 112640 / 1000000\n",
            "generated 122880 / 1000000\n",
            "generated 133120 / 1000000\n",
            "generated 143360 / 1000000\n",
            "generated 153600 / 1000000\n",
            "generated 163840 / 1000000\n",
            "generated 174080 / 1000000\n",
            "generated 184320 / 1000000\n",
            "generated 194560 / 1000000\n",
            "generated 204800 / 1000000\n",
            "generated 215040 / 1000000\n",
            "generated 225280 / 1000000\n",
            "generated 235520 / 1000000\n",
            "generated 245760 / 1000000\n",
            "generated 256000 / 1000000\n",
            "generated 266240 / 1000000\n",
            "generated 276480 / 1000000\n",
            "generated 286720 / 1000000\n",
            "generated 296960 / 1000000\n",
            "generated 307200 / 1000000\n",
            "generated 317440 / 1000000\n",
            "generated 327680 / 1000000\n",
            "generated 337920 / 1000000\n",
            "generated 348160 / 1000000\n",
            "generated 358400 / 1000000\n",
            "generated 368640 / 1000000\n",
            "generated 378880 / 1000000\n",
            "generated 389120 / 1000000\n",
            "generated 399360 / 1000000\n",
            "generated 409600 / 1000000\n",
            "generated 419840 / 1000000\n",
            "generated 430080 / 1000000\n",
            "generated 440320 / 1000000\n",
            "generated 450560 / 1000000\n",
            "generated 460800 / 1000000\n",
            "generated 471040 / 1000000\n",
            "generated 481280 / 1000000\n",
            "generated 491520 / 1000000\n",
            "generated 501760 / 1000000\n",
            "generated 512000 / 1000000\n",
            "generated 522240 / 1000000\n",
            "generated 532480 / 1000000\n",
            "generated 542720 / 1000000\n",
            "generated 552960 / 1000000\n",
            "generated 563200 / 1000000\n",
            "generated 573440 / 1000000\n",
            "generated 583680 / 1000000\n",
            "generated 593920 / 1000000\n",
            "generated 604160 / 1000000\n",
            "generated 614400 / 1000000\n",
            "generated 624640 / 1000000\n",
            "generated 634880 / 1000000\n",
            "generated 645120 / 1000000\n",
            "generated 655360 / 1000000\n",
            "generated 665600 / 1000000\n",
            "generated 675840 / 1000000\n",
            "generated 686080 / 1000000\n",
            "generated 696320 / 1000000\n",
            "generated 706560 / 1000000\n",
            "generated 716800 / 1000000\n",
            "generated 727040 / 1000000\n",
            "generated 737280 / 1000000\n",
            "generated 747520 / 1000000\n",
            "generated 757760 / 1000000\n",
            "generated 768000 / 1000000\n",
            "generated 778240 / 1000000\n",
            "generated 788480 / 1000000\n",
            "generated 798720 / 1000000\n",
            "generated 808960 / 1000000\n",
            "generated 819200 / 1000000\n",
            "generated 829440 / 1000000\n",
            "generated 839680 / 1000000\n",
            "generated 849920 / 1000000\n",
            "generated 860160 / 1000000\n",
            "generated 870400 / 1000000\n",
            "generated 880640 / 1000000\n",
            "generated 890880 / 1000000\n",
            "generated 901120 / 1000000\n",
            "generated 911360 / 1000000\n",
            "generated 921600 / 1000000\n",
            "generated 931840 / 1000000\n",
            "generated 942080 / 1000000\n",
            "generated 952320 / 1000000\n",
            "generated 962560 / 1000000\n",
            "generated 972800 / 1000000\n",
            "generated 983040 / 1000000\n",
            "generated 993280 / 1000000\n",
            "done generating -> /content/drive/MyDrive/pgtcn_model/generated_streamed.txt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "def sample_topk_batch(model, stoi, itos, batch_size=512, seq_len=20, temperature=0.7, top_k=20):\n",
        "    \"\"\"\n",
        "    Generates `batch_size` passwords in parallel, each up to seq_len.\n",
        "    Returns list of strings.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    vocab_size = len(stoi)\n",
        "    # initialize with a random start token index for each sequence\n",
        "    x = torch.randint(0, vocab_size, (batch_size, 1), dtype=torch.long, device=device)\n",
        "    finished = [False] * batch_size\n",
        "    outputs = [[] for _ in range(batch_size)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t in range(seq_len):\n",
        "            logits = model(x)  # shape: (batch_size, seq_len_sofar, vocab)\n",
        "            last_logits = logits[:, -1, :]  # (batch_size, vocab)\n",
        "            # apply temperature\n",
        "            last_logits = last_logits / temperature\n",
        "            probs = F.softmax(last_logits, dim=-1)\n",
        "\n",
        "            # top-k sampling per batch item\n",
        "            top_probs, top_idx = torch.topk(probs, k=top_k, dim=-1)  # (batch_size, top_k)\n",
        "            # sample indices from top_probs\n",
        "            samp = torch.multinomial(top_probs, 1).squeeze(1)  # (batch_size,)\n",
        "            idx = top_idx[torch.arange(batch_size), samp]  # chosen token ids (batch_size,)\n",
        "\n",
        "            # append chars and update finished flags\n",
        "            for i in range(batch_size):\n",
        "                ch = itos[idx[i].item()]\n",
        "                # if you have an <EOS> token, handle it; otherwise we treat all chars as regular\n",
        "                if ch == \"<EOS>\":\n",
        "                    finished[i] = True\n",
        "                else:\n",
        "                    outputs[i].append(ch)\n",
        "            # prepare next input x by concatenating chosen idx\n",
        "            x = torch.cat([x, idx.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # stop early if all finished\n",
        "            if all(finished):\n",
        "                break\n",
        "\n",
        "    # join outputs and fallback for empties\n",
        "    pwds = [''.join(chars) if len(chars)>0 else random.choice(list(stoi.keys())) for chars in outputs]\n",
        "    return pwds\n",
        "\n",
        "# streaming 1M generation example (adjust N and batch_size as you wish)\n",
        "out_path = Path(\"/content/drive/MyDrive/pgtcn_model/generated_streamed.txt\")\n",
        "N = 1_000_000\n",
        "batch_size = 1024   # try 512/1024 depending on available memory\n",
        "seq_len = 20\n",
        "temperature = 0.7\n",
        "top_k = 20\n",
        "\n",
        "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    generated_count = 0\n",
        "    while generated_count < N:\n",
        "        b = min(batch_size, N - generated_count)\n",
        "        pwds = sample_topk_batch(model, stoi, itos, batch_size=b, seq_len=seq_len,\n",
        "                                 temperature=temperature, top_k=top_k)\n",
        "        for p in pwds:\n",
        "            fout.write(p + \"\\n\")\n",
        "        generated_count += b\n",
        "        # optional: print progress every X batches\n",
        "        if generated_count % (batch_size * 10) == 0:\n",
        "            print(f\"generated {generated_count} / {N}\")\n",
        "print(\"done generating ->\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l4XDQXJ4sV2",
        "outputId": "f315cdee-e92e-4389-e1cc-e762070f087e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set size: 7426\n",
            "Generated: 3368950\n",
            "Matches: 661 / 7426\n",
            "Match rate: 8.9012%\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”¹ Load your test set again (20% split, like before)\n",
        "with open(\"/content/drive/MyDrive/pgtcn_model/myspace.txt\") as f:\n",
        "    data = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "import random\n",
        "random.shuffle(data)\n",
        "split = int(0.8 * len(data))   # 80/20 split\n",
        "test_set = set(data[split:])\n",
        "\n",
        "print(f\"Test set size: {len(test_set)}\")\n",
        "\n",
        "\n",
        "matches = set()\n",
        "count = 0\n",
        "\n",
        "with open(\"/content/drive/MyDrive/pgtcn_model/generated_streamed.txt\") as f:\n",
        "    for line in f:\n",
        "        pwd = line.strip()\n",
        "        count += 1\n",
        "        if pwd in test_set:\n",
        "            matches.add(pwd)\n",
        "\n",
        "print(f\"Generated: {count}\")\n",
        "print(f\"Matches: {len(matches)} / {len(test_set)}\")\n",
        "print(f\"Match rate: {len(matches)/len(test_set):.4%}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
