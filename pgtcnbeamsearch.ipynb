{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13668533,"sourceType":"datasetVersion","datasetId":8690756},{"sourceId":13668569,"sourceType":"datasetVersion","datasetId":8690783},{"sourceId":13729185,"sourceType":"datasetVersion","datasetId":8734911},{"sourceId":13729195,"sourceType":"datasetVersion","datasetId":8691921}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"mkdir -p pgtcn_modelMulti/{000webhost,10-million-password-list-top-1000000,Ashley-Madison,hashkiller-dict,hashmob.net.large.found,honeynet,hotmail,myspace,NordVPN,phpbb,rockyou_utf8,singles.org}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:47.027253Z","iopub.execute_input":"2025-11-26T07:44:47.027854Z","iopub.status.idle":"2025-11-26T07:44:47.147640Z","shell.execute_reply.started":"2025-11-26T07:44:47.027830Z","shell.execute_reply":"2025-11-26T07:44:47.146694Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Imports and Setup\n\"\"\"\nENHANCED PGTCN - Multi-Strategy Password Generation\nGPU-Optimized for CUDA 12.8 with RTX 4500 Ada\nImplements 5 key improvements to achieve 20-40% match rate\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport numpy as np\nfrom pathlib import Path\nimport json\nimport math\nfrom tqdm import tqdm\n\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nfrom collections import Counter\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:47.149421Z","iopub.execute_input":"2025-11-26T07:44:47.149641Z","iopub.status.idle":"2025-11-26T07:44:50.900028Z","shell.execute_reply.started":"2025-11-26T07:44:47.149619Z","shell.execute_reply":"2025-11-26T07:44:50.899276Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 2: GPU Monitoring Functions\n# ============================================\n# GPU MONITORING & VERIFICATION\n# ============================================\n\ndef check_gpu_status():\n    \"\"\"Check and display GPU status\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"GPU STATUS CHECK\")\n    print(\"=\"*80)\n    \n    if torch.cuda.is_available():\n        print(f\"‚úÖ CUDA Available: Yes\")\n        print(f\"üéÆ GPU Device: {torch.cuda.get_device_name(0)}\")\n        print(f\"üî¢ CUDA Version: {torch.version.cuda}\")\n        print(f\"üíæ Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n        print(f\"üîß Device Capability: {torch.cuda.get_device_capability(0)}\")\n        print(f\"üìä Number of GPUs: {torch.cuda.device_count()}\")\n        print(f\"üéØ Current Device: {torch.cuda.current_device()}\")\n        \n        # Enable optimizations\n        if hasattr(torch.backends.cuda, 'matmul'):\n            torch.backends.cuda.matmul.allow_tf32 = True\n            print(f\"üöÄ TF32 enabled for matrix multiplication\")\n        \n        if hasattr(torch.backends.cudnn, 'allow_tf32'):\n            torch.backends.cudnn.allow_tf32 = True\n            print(f\"üöÄ TF32 enabled for cuDNN\")\n        \n        torch.backends.cudnn.benchmark = True\n        print(f\"üöÄ cuDNN benchmark mode enabled\")\n        \n        print(\"=\"*80 + \"\\n\")\n        return True\n    else:\n        print(\"‚ùå CUDA NOT AVAILABLE - Will run on CPU\")\n        print(\"=\"*80 + \"\\n\")\n        return False\n\n\ndef monitor_gpu_memory():\n    \"\"\"Monitor GPU memory usage during training\"\"\"\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated(0) / 1e9\n        reserved = torch.cuda.memory_reserved(0) / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        free = total - reserved\n        \n        print(f\"\\n{'='*60}\")\n        print(\"GPU MEMORY STATUS\")\n        print(f\"{'='*60}\")\n        print(f\"Allocated:  {allocated:6.2f} GB ({allocated/total*100:5.1f}%)\")\n        print(f\"Reserved:   {reserved:6.2f} GB ({reserved/total*100:5.1f}%)\")\n        print(f\"Free:       {free:6.2f} GB ({free/total*100:5.1f}%)\")\n        print(f\"Total:      {total:6.2f} GB\")\n        print(f\"{'='*60}\\n\")\n\n\ndef clear_gpu_cache():\n    \"\"\"Clear GPU cache to free up memory\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        print(\"‚úì GPU cache cleared\")\n\n\n# Initialize GPU at startup\nprint(\"Initializing GPU...\")\nGPU_AVAILABLE = check_gpu_status()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:50.900944Z","iopub.execute_input":"2025-11-26T07:44:50.901302Z","iopub.status.idle":"2025-11-26T07:44:51.014861Z","shell.execute_reply.started":"2025-11-26T07:44:50.901283Z","shell.execute_reply":"2025-11-26T07:44:51.014295Z"}},"outputs":[{"name":"stdout","text":"Initializing GPU...\n\n================================================================================\nGPU STATUS CHECK\n================================================================================\n‚úÖ CUDA Available: Yes\nüéÆ GPU Device: Tesla T4\nüî¢ CUDA Version: 12.4\nüíæ Total VRAM: 15.83 GB\nüîß Device Capability: (7, 5)\nüìä Number of GPUs: 2\nüéØ Current Device: 0\nüöÄ TF32 enabled for matrix multiplication\nüöÄ TF32 enabled for cuDNN\nüöÄ cuDNN benchmark mode enabled\n================================================================================\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class Config:\n    # Dataset configurations\n    DATASETS = {\n        # '000webhost': {\n        #     'path': 'datasets/000webhost.txt',\n        #     'model_dir': 'pgtcn_modelMulti/000webhost/',\n        #     'description': '000webhost password leak'\n        # },\n        # '10-million-password-list-top-1000000': {\n        #     'path': 'datasets/10-million-password-list-top-1000000.txt',\n        #     'model_dir': 'pgtcn_modelMulti/10-million-password-list-top-1000000/',\n        #     'description': '10 million password list top 1M'\n        # },\n        # 'Ashley-Madison': {\n        #     'path': 'datasets/Ashley-Madison.txt',\n        #     'model_dir': 'pgtcn_modelMulti/Ashley-Madiso/',\n        #     'description': 'Ashley Madison password leak'\n        # },\n        # 'hashkiller-dict': {\n        #     'path': 'datasets/hashkiller-dict.txt',\n        #     'model_dir': 'pgtcn_modelMulti/hashkiller-dict/',\n        #     'description': 'Hashkiller dictionary'\n        # },\n        # 'hashmob.net.large.found': {\n        #     'path': 'datasets/hashmob.net.large.found.txt',\n        #     'model_dir': 'pgtcn_modelMulti/hashmob.net.large.found/',\n        #     'description': 'Hashmob large found'\n        # },\n        # 'honeynet': {\n        #     'path': 'datasets/honeynet.txt',\n        #     'model_dir': 'pgtcn_modelMulti/honeynet/',\n        #     'description': 'Honeynet password leak'\n        # },\n        # 'hotmail': {\n        #     'path': 'datasets/hotmail.txt',\n        #     'model_dir': 'pgtcn_modelMulti/hotmail/',\n        #     'description': 'Hotmail password leak'\n        # },\n        'myspace': {\n            'path': '/kaggle/input/capstonedatasets/myspace.txt',\n            'model_dir': 'pgtcn_modelMulti/myspace/',\n            'description': 'MySpace password leak'}\n        # },\n        # 'NordVPN': {\n        #     'path': 'datasets/NordVPN.txt',\n        #     'model_dir': 'pgtcn_modelMulti/NordVPN/',\n        #     'description': 'NordVPN password leak'\n        # },\n        # 'phpbb': {\n        #     'path': 'datasets/phpbb.txt',\n        #     'model_dir': 'pgtcn_modelMulti/phpbb/',\n        #     'description': 'phpBB password leak'\n        # },\n        # 'rockyou': {\n        #     'path': 'datasets/rockyou.txt',\n        #     'model_dir': 'pgtcn_modelMulti/rockyou/',\n        #     'description': 'RockYou password leak (14M passwords)'\n        # },\n        # 'singles.org': {\n        #     'path': 'datasets/singles.org.txt',\n        #     'model_dir': 'pgtcn_modelMulti/singles.org/',\n        #     'description': 'Singles.org password leak'\n        # }\n    }\n    \n    # Model architecture\n    EMBEDDING_DIM = 512\n    TCN_CHANNELS = [512, 512, 512, 512, 512, 512]\n    KERNEL_SIZE = 3\n    DROPOUT = 0.1\n    \n    # Training parameters\n    BATCH_SIZE = 64\n    LEARNING_RATE = 3e-4\n    EPOCHS = 20  # Increased for better convergence\n    WEIGHT_DECAY = 1e-6\n    GRAD_CLIP = 0.5\n    USE_AMP = True\n    USE_FOCAL_LOSS = True\n    FOCAL_ALPHA = 0.25\n    FOCAL_GAMMA = 2.0\n    LABEL_SMOOTHING = 0.1\n    \n    # Data parameters\n    SEQ_LEN = 18  # Increased to accommodate SOS and EOS tokens\n    NUM_WORKERS = 4\n    PIN_MEMORY = True\n    \n    # Device\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Dynamic\n    VOCAB_SIZE = None\n    DATA_PATH = None\n    MODEL_DIR = None\n    # Generation\n    BEAM_WIDTH = 5\n    LENGTH_PENALTY = 0.8\n    \n    @staticmethod\n    def set_dataset(dataset_name):\n        if dataset_name not in Config.DATASETS:\n            raise ValueError(f\"Dataset {dataset_name} not found\")\n        Config.DATA_PATH = Config.DATASETS[dataset_name]['path']\n        Config.MODEL_DIR = Config.DATASETS[dataset_name]['model_dir']\n    \n    @staticmethod\n    def get_available_datasets():\n        return list(Config.DATASETS.keys())\n    \n    @staticmethod\n    def print_config():\n        print(\"\\n\" + \"=\"*60)\n        print(\"CONFIGURATION\")\n        print(\"=\"*60)\n        print(f\"Embedding Dim:     {Config.EMBEDDING_DIM}\")\n        print(f\"TCN Channels:      {Config.TCN_CHANNELS}\")\n        print(f\"Kernel Size:       {Config.KERNEL_SIZE}\")\n        print(f\"Dropout:           {Config.DROPOUT}\")\n        print(f\"Batch Size:        {Config.BATCH_SIZE}\")\n        print(f\"Learning Rate:     {Config.LEARNING_RATE}\")\n        print(f\"Epochs:            {Config.EPOCHS}\")\n        print(f\"Device:            {Config.DEVICE}\")\n        print(f\"Use AMP:           {Config.USE_AMP}\")\n        print(f\"Use Focal Loss:    {Config.USE_FOCAL_LOSS}\")\n        print(\"=\"*60 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.015594Z","iopub.execute_input":"2025-11-26T07:44:51.015830Z","iopub.status.idle":"2025-11-26T07:44:51.024326Z","shell.execute_reply.started":"2025-11-26T07:44:51.015803Z","shell.execute_reply":"2025-11-26T07:44:51.023642Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Multi-Dataset Utilities\n# ============================================\n# MULTI-DATASET UTILITIES\n# ============================================\n\n\ndef save_vocab_files(stoi, itos, target_dir):\n    \"\"\"Persist stoi/itos dictionaries as separate JSON files.\"\"\"\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n    stoi_path = target_path / \"stoi.json\"\n    itos_path = target_path / \"itos.json\"\n\n    with open(stoi_path, 'w') as f:\n        json.dump(stoi, f, indent=2)\n\n    with open(itos_path, 'w') as f:\n        json.dump({str(k): v for k, v in itos.items()}, f, indent=2)\n\n    return str(stoi_path), str(itos_path)\n\n\ndef load_vocab_from_files(stoi_path, itos_path):\n    \"\"\"Load stoi/itos dictionaries from disk.\"\"\"\n    with open(stoi_path, 'r') as f:\n        stoi = json.load(f)\n\n    with open(itos_path, 'r') as f:\n        raw_itos = json.load(f)\n        itos = {int(k): v for k, v in raw_itos.items()}\n\n    return stoi, itos\n\n\ndef load_dataset(dataset_name):\n    \"\"\"Load and prepare a specific dataset\"\"\"\n    Config.set_dataset(dataset_name)\n\n    print(f\"\\n{'='*80}\")\n    print(f\"Loading {Config.DATASETS[dataset_name]['description']}\")\n    print(f\"{'='*80}\\n\")\n\n    try:\n        with open(Config.DATA_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n            passwords = [line.strip() for line in f if line.strip()]\n\n        filtered = [pwd for pwd in passwords if 4 <= len(pwd) <= 16 and pwd.isascii()]\n        print(f\"‚úì Loaded {len(passwords):,} passwords\")\n        print(f\"‚úì Filtered: {len(filtered):,} passwords (4-16 chars, ASCII only)\\n\")\n\n        return filtered\n\n    except FileNotFoundError:\n        print(f\"‚ùå Error: Dataset file not found at {Config.DATA_PATH}\")\n        return None\n    except Exception as e:\n        print(f\"‚ùå Error loading dataset: {str(e)}\")\n        return None\n\n\ndef save_dataset_model(model, stoi, itos, dataset_name, epoch=None):\n    \"\"\"Save model and vocabulary for a specific dataset\"\"\"\n    model_dir = Config.DATASETS[dataset_name]['model_dir']\n    Path(model_dir).mkdir(parents=True, exist_ok=True)\n\n    # Save model\n    if epoch is not None:\n        model_path = f\"{model_dir}/pgtcn_epoch_{epoch}.pt\"\n    else:\n        model_path = f\"{model_dir}/pgtcn_final.pt\"\n\n    torch.save(model.state_dict(), model_path)\n\n    # Save vocabulary bundle\n    vocab_path = f\"{model_dir}/vocabulary.json\"\n    vocab_payload = {\n        'stoi': stoi,\n        'itos': {str(k): v for k, v in itos.items()}\n    }\n    with open(vocab_path, 'w') as f:\n        json.dump(vocab_payload, f, indent=2)\n\n    # Save separate stoi/itos files for downstream use\n    stoi_path, itos_path = save_vocab_files(stoi, itos, model_dir)\n\n    # Save config\n    config_path = f\"{model_dir}/config.json\"\n    config_data = {\n        'dataset': dataset_name,\n        'vocab_size': len(stoi),\n        'embedding_dim': Config.EMBEDDING_DIM,\n        'tcn_channels': Config.TCN_CHANNELS,\n        'kernel_size': Config.KERNEL_SIZE,\n        'dropout': Config.DROPOUT,\n        'seq_len': Config.SEQ_LEN\n    }\n    with open(config_path, 'w') as f:\n        json.dump(config_data, f, indent=2)\n\n    print(f\"‚úì Model saved: {model_path}\")\n    print(f\"‚úì Vocabulary bundle saved: {vocab_path}\")\n    print(f\"‚úì STOI file saved: {stoi_path}\")\n    print(f\"‚úì ITOS file saved: {itos_path}\")\n    print(f\"‚úì Config saved: {config_path}\")\n\n    return model_path\n\n\ndef load_dataset_model(dataset_name, vocab_size):\n    \"\"\"Load a trained model for a specific dataset\"\"\"\n    model_dir = Config.DATASETS[dataset_name]['model_dir']\n    model_path = f\"{model_dir}/pgtcn_final.pt\"\n    vocab_path = f\"{model_dir}/vocabulary.json\"\n    stoi_path = Path(model_dir) / \"stoi.json\"\n    itos_path = Path(model_dir) / \"itos.json\"\n\n    # Load vocabulary (prefer dedicated files, fall back to bundle)\n    if stoi_path.exists() and itos_path.exists():\n        stoi, itos = load_vocab_from_files(stoi_path, itos_path)\n    else:\n        with open(vocab_path, 'r') as f:\n            vocab_data = json.load(f)\n            stoi = vocab_data['stoi']\n            itos = {int(k): v for k, v in vocab_data['itos'].items()}\n\n    # Create model\n    model = PGTCN(\n        vocab_size=vocab_size,\n        embedding_dim=Config.EMBEDDING_DIM,\n        num_channels=Config.TCN_CHANNELS,\n        kernel_size=Config.KERNEL_SIZE,\n        dropout=Config.DROPOUT\n    ).to(Config.DEVICE)\n\n    # Load weights\n    state_dict = torch.load(model_path, map_location=Config.DEVICE)\n    model.load_state_dict(state_dict)\n\n    print(f\"‚úì Model loaded from: {model_path}\")\n\n    return model, stoi, itos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.025942Z","iopub.execute_input":"2025-11-26T07:44:51.026157Z","iopub.status.idle":"2025-11-26T07:44:51.046317Z","shell.execute_reply.started":"2025-11-26T07:44:51.026115Z","shell.execute_reply":"2025-11-26T07:44:51.045697Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cell 5: Vocabulary\n# ============================================\n# VOCABULARY\n# ============================================\n\ndef build_vocabulary(passwords):\n    unique_chars = set()\n    for pwd in passwords:\n        unique_chars.update(pwd)\n    \n    unique_chars = sorted(unique_chars)\n    \n    stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n    \n    for idx, char in enumerate(unique_chars, start=4):\n        stoi[char] = idx\n    \n    itos = {v: k for k, v in stoi.items()}\n    return stoi, itos\n\n\ndef tokenize_password(pwd, stoi, max_len=18):\n    # Add SOS and EOS tokens\n    tokens = [stoi.get(\"<SOS>\", 1)] + [stoi.get(c, stoi.get(\"<UNK>\", 3)) for c in pwd] + [stoi.get(\"<EOS>\", 2)]\n    \n    # Truncate if necessary (keeping SOS and EOS is important, but we prioritize fitting in max_len)\n    if len(tokens) > max_len:\n        tokens = tokens[:max_len-1] + [stoi.get(\"<EOS>\", 2)]\n    \n    # Pad\n    tokens += [stoi.get(\"<PAD>\", 0)] * (max_len - len(tokens))\n    return tokens[:max_len]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.047046Z","iopub.execute_input":"2025-11-26T07:44:51.047328Z","iopub.status.idle":"2025-11-26T07:44:51.062011Z","shell.execute_reply.started":"2025-11-26T07:44:51.047305Z","shell.execute_reply":"2025-11-26T07:44:51.061500Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 6: Dataset with Augmentation\n# ============================================\n# ENHANCED DATASET WITH AUGMENTATION\n# ============================================\n\nclass PasswordDataset(Dataset):\n    def __init__(self, passwords, stoi, max_len=16, augment=True):\n        self.passwords = passwords\n        self.stoi = stoi\n        self.max_len = max_len\n        self.augment = augment\n    \n    def __len__(self):\n        return len(self.passwords)\n    \n    def __getitem__(self, idx):\n        pwd = self.passwords[idx]\n        \n        # Data augmentation - random case flipping\n        if self.augment and random.random() < 0.1:\n            pwd = pwd.swapcase()\n        \n        tokens = tokenize_password(pwd, self.stoi, self.max_len)\n        return torch.tensor(tokens, dtype=torch.long)\n\n\ndef collate_fn(batch):\n    return torch.stack(batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.062693Z","iopub.execute_input":"2025-11-26T07:44:51.062944Z","iopub.status.idle":"2025-11-26T07:44:51.075732Z","shell.execute_reply.started":"2025-11-26T07:44:51.062919Z","shell.execute_reply":"2025-11-26T07:44:51.075077Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 7: TCN Building Blocks\n# ============================================\n# TCN BUILDING BLOCKS\n# ============================================\n\nclass Chomp1d(nn.Module):\n    \"\"\"Remove padding from the end of sequence\"\"\"\n    def __init__(self, chomp_size):\n        super().__init__()\n        self.chomp_size = chomp_size\n    \n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n\n\nclass TemporalBlock(nn.Module):\n    \"\"\"Temporal block with dilated causal convolutions\"\"\"\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super().__init__()\n        \n        # First convolution\n        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                               stride=stride, padding=padding, dilation=dilation)\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n        \n        # Second convolution\n        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                               stride=stride, padding=padding, dilation=dilation)\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n        \n        # Residual connection\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # First conv block\n        out = self.conv1(x)\n        out = self.chomp1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        \n        # Second conv block\n        out = self.conv2(out)\n        out = self.chomp2(out)\n        out = self.relu2(out)\n        out = self.dropout2(out)\n        \n        # Residual connection (NON-INPLACE!)\n        res = x if self.downsample is None else self.downsample(x)\n        out = out + res  # Non-inplace addition\n        \n        return self.relu(out)\n\n\nclass TemporalConvNet(nn.Module):\n    \"\"\"Temporal Convolutional Network (TCN)\"\"\"\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super().__init__()\n        layers = []\n        num_levels = len(num_channels)\n        \n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            \n            layers.append(TemporalBlock(\n                in_channels, out_channels, kernel_size, stride=1,\n                dilation=dilation_size,\n                padding=(kernel_size-1) * dilation_size,\n                dropout=dropout\n            ))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.076486Z","iopub.execute_input":"2025-11-26T07:44:51.076678Z","iopub.status.idle":"2025-11-26T07:44:51.093846Z","shell.execute_reply.started":"2025-11-26T07:44:51.076662Z","shell.execute_reply":"2025-11-26T07:44:51.093157Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Cell 8: PGTCN Model\n# ============================================\n# ENHANCED PGTCN MODEL\n# ============================================\n\nclass PGTCN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=256, num_channels=[256]*6, \n                 kernel_size=3, dropout=0.1):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # Embedding with better initialization\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n        self.embedding.weight.data[0].zero_()\n        \n        # TCN\n        self.tcn = TemporalConvNet(\n            num_inputs=embedding_dim,\n            num_channels=num_channels,\n            kernel_size=kernel_size,\n            dropout=dropout\n        )\n        \n        # Layer normalization for stability\n        self.layer_norm = nn.LayerNorm(num_channels[-1])\n        \n        # Output projection\n        self.fc = nn.Linear(num_channels[-1], vocab_size)\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        \n    def forward(self, x):\n        emb = self.embedding(x).transpose(1, 2)\n        tcn_out = self.tcn(emb)\n        tcn_out = tcn_out.transpose(1, 2)\n        tcn_out = self.layer_norm(tcn_out)\n        logits = self.fc(tcn_out)\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.094594Z","iopub.execute_input":"2025-11-26T07:44:51.094826Z","iopub.status.idle":"2025-11-26T07:44:51.111682Z","shell.execute_reply.started":"2025-11-26T07:44:51.094805Z","shell.execute_reply":"2025-11-26T07:44:51.111052Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Cell 9: Focal Loss\n# ============================================\n# FOCAL LOSS FOR HARD EXAMPLES\n# ============================================\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0, ignore_index=-100):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n    \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none', ignore_index=self.ignore_index)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.112354Z","iopub.execute_input":"2025-11-26T07:44:51.112631Z","iopub.status.idle":"2025-11-26T07:44:51.131159Z","shell.execute_reply.started":"2025-11-26T07:44:51.112616Z","shell.execute_reply":"2025-11-26T07:44:51.130526Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Cell 10: Nucleus Sampling\n# ============================================\n# NUCLEUS (TOP-P) SAMPLING\n# ============================================\n\ndef nucleus_sampling(logits, top_p=0.9, temperature=0.9):\n    \"\"\"Sample from nucleus (top-p) distribution\"\"\"\n    logits = logits / temperature\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n    \n    # Remove tokens with cumulative probability above threshold\n    sorted_indices_to_remove = cumulative_probs > top_p\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n    sorted_indices_to_remove[..., 0] = 0\n    \n    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n    logits[indices_to_remove] = float('-inf')\n    \n    probs = F.softmax(logits, dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n    return next_token.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.131987Z","iopub.execute_input":"2025-11-26T07:44:51.132221Z","iopub.status.idle":"2025-11-26T07:44:51.148959Z","shell.execute_reply.started":"2025-11-26T07:44:51.132205Z","shell.execute_reply":"2025-11-26T07:44:51.148261Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class DiversePasswordGenerator:\n    def __init__(self, model, stoi, itos, device='cuda'):\n        self.model = model\n        self.stoi = stoi\n        self.itos = itos\n        self.device = device\n        self.generated_cache = set()\n    \n    @torch.no_grad()\n    def generate_with_nucleus(self, max_len=16, temperature=0.9, top_p=0.9):\n        \"\"\"Generate with nucleus sampling for diversity\"\"\"\n        self.model.eval()\n        \n        x = torch.tensor([[self.stoi[\"<SOS>\"]]], dtype=torch.long, device=self.device)\n        password = \"\"\n        \n        for _ in range(max_len - 1):\n            logits = self.model(x)\n            next_logits = logits[0, -1, :]\n            \n            next_token = nucleus_sampling(next_logits, top_p=top_p, temperature=temperature)\n            \n            if next_token == self.stoi[\"<EOS>\"]:\n                break\n            \n            char = self.itos.get(next_token, \"\")\n            if char and char not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:\n                password += char\n            \n            x = torch.cat([x, torch.tensor([[next_token]], dtype=torch.long, device=self.device)], dim=1)\n        \n        return password.strip()\n    \n    @torch.no_grad()\n    def generate_with_beam_search(self, max_len=16, beam_width=5, length_penalty=0.8):\n        \"\"\"Generate with beam search for higher quality\"\"\"\n        self.model.eval()\n        \n        # Initialize beams: (sequence, score, length)\n        beams = [([self.stoi[\"<SOS>\"]], 0.0, 1)]\n        \n        for _ in range(max_len - 1):\n            new_beams = []\n            \n            for seq, score, length in beams:\n                if seq[-1] == self.stoi[\"<EOS>\"]:\n                    new_beams.append((seq, score, length))\n                    continue\n                \n                x = torch.tensor([seq], dtype=torch.long, device=self.device)\n                logits = self.model(x)\n                next_logits = logits[0, -1, :]\n                \n                # Get top-k candidates\n                probs = F.softmax(next_logits, dim=-1)\n                top_probs, top_indices = torch.topk(probs, beam_width)\n                \n                for prob, idx in zip(top_probs, top_indices):\n                    new_seq = seq + [idx.item()]\n                    new_score = score + torch.log(prob).item()\n                    # Apply length penalty\n                    penalized_score = new_score / (length ** length_penalty)\n                    new_beams.append((new_seq, penalized_score, length + 1))\n            \n            # Keep top beam_width beams\n            new_beams.sort(key=lambda x: x[1], reverse=True)\n            beams = new_beams[:beam_width]\n        \n        # Choose the best beam\n        best_seq, _, _ = max(beams, key=lambda x: x[1])\n        \n        # Convert to password\n        password = \"\"\n        for token in best_seq[1:]:  # Skip <SOS>\n            if token == self.stoi[\"<EOS>\"]:\n                break\n            char = self.itos.get(token, \"\")\n            if char and char not in ['<PAD>', '<SOS>', '<EOS>', '<UNK>']:\n                password += char\n        \n        return password.strip()\n    \n    def generate_diverse_batch(self, num_passwords=10000, max_attempts_per_pwd=5, use_beam_search=False):\n        \"\"\"Generate diverse passwords with deduplication\"\"\"\n        passwords = []\n        temperature_range = [0.7, 0.8, 0.9, 1.0]\n        top_p_range = [0.85, 0.90, 0.95]\n        \n        pbar = tqdm(total=num_passwords, desc=\"Generating diverse passwords\")\n        \n        attempts = 0\n        max_total_attempts = num_passwords * max_attempts_per_pwd\n        \n        while len(passwords) < num_passwords and attempts < max_total_attempts:\n            if use_beam_search:\n                pwd = self.generate_with_beam_search(max_len=Config.SEQ_LEN, beam_width=Config.BEAM_WIDTH, length_penalty=Config.LENGTH_PENALTY)\n            else:\n                # Vary temperature and top_p for diversity\n                temp = random.choice(temperature_range)\n                top_p = random.choice(top_p_range)\n                pwd = self.generate_with_nucleus(max_len=Config.SEQ_LEN, temperature=temp, top_p=top_p)\n            \n            # Only add unique passwords\n            if pwd and pwd not in self.generated_cache and len(pwd) >= 4:\n                passwords.append(pwd)\n                self.generated_cache.add(pwd)\n                pbar.update(1)\n            \n            attempts += 1\n        \n        pbar.close()\n        \n        return passwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.149598Z","iopub.execute_input":"2025-11-26T07:44:51.149759Z","iopub.status.idle":"2025-11-26T07:44:51.163521Z","shell.execute_reply.started":"2025-11-26T07:44:51.149746Z","shell.execute_reply":"2025-11-26T07:44:51.162987Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Cell 12: Enhanced Trainer\n# ============================================\n# ENHANCED TRAINER WITH GPU OPTIMIZATION\n# ============================================\n\nclass PGTCNTrainer:\n    def __init__(self, model, config, stoi, itos):\n        self.model = model\n        self.config = config\n        self.stoi = stoi\n        self.itos = itos\n        self.device = config.DEVICE\n        \n        self.optimizer = optim.AdamW(\n            model.parameters(),\n            lr=config.LEARNING_RATE,\n            weight_decay=config.WEIGHT_DECAY,\n            betas=(0.9, 0.999)\n        )\n        \n        # Cosine annealing with warmup\n        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            self.optimizer,\n            T_0=10,\n            T_mult=2,\n            eta_min=1e-6\n        )\n        \n        self.scaler = torch.amp.GradScaler('cuda') if config.USE_AMP else None\n        \n        # Use focal loss if enabled\n        if config.USE_FOCAL_LOSS:\n            self.criterion = FocalLoss(\n                alpha=config.FOCAL_ALPHA,\n                gamma=config.FOCAL_GAMMA,\n                ignore_index=stoi[\"<PAD>\"]\n            )\n        else:\n            self.criterion = None\n        \n        self.best_loss = float('inf')\n        self.history = []\n    \n    def train_epoch(self, dataloader):\n        self.model.train()\n        epoch_losses = []\n        \n        pbar = tqdm(dataloader, desc=\"Training\")\n        for batch in pbar:\n            input_ids = batch.to(self.device)\n            \n            with torch.amp.autocast('cuda', enabled=self.config.USE_AMP):\n                logits = self.model(input_ids)\n                shift_logits = logits[:, :-1, :].contiguous()\n                shift_labels = input_ids[:, 1:].contiguous()\n                \n                if self.criterion:\n                    loss = self.criterion(\n                        shift_logits.view(-1, self.config.VOCAB_SIZE),\n                        shift_labels.view(-1)\n                    )\n                else:\n                    loss = F.cross_entropy(\n                        shift_logits.view(-1, self.config.VOCAB_SIZE),\n                        shift_labels.view(-1),\n                        ignore_index=self.stoi[\"<PAD>\"],\n                        label_smoothing=self.config.LABEL_SMOOTHING\n                    )\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            epoch_losses.append(loss.item())\n            \n            self.optimizer.zero_grad()\n            \n            if self.scaler:\n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRAD_CLIP)\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.GRAD_CLIP)\n                self.optimizer.step()\n            \n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        return np.mean(epoch_losses) if epoch_losses else float('inf')\n    \n    def train(self, dataloader):\n        print(f\"\\n{'='*80}\")\n        print(f\"Training ENHANCED PGTCN for {self.config.EPOCHS} epochs\")\n        print(f\"{'='*80}\\n\")\n        \n        for epoch in range(self.config.EPOCHS):\n            avg_loss = self.train_epoch(dataloader)\n            \n            if avg_loss < self.best_loss:\n                self.best_loss = avg_loss\n                # Save best model\n                torch.save(self.model.state_dict(), \n                          f\"{self.config.MODEL_DIR}/pgtcn_best.pt\")\n            \n            self.history.append({\n                'epoch': epoch + 1,\n                'loss': avg_loss,\n                'lr': self.optimizer.param_groups[0]['lr']\n            })\n            \n            print(f\"Epoch {epoch+1}/{self.config.EPOCHS} | Loss: {avg_loss:.4f} | \"\n                  f\"Best: {self.best_loss:.4f} | LR: {self.optimizer.param_groups[0]['lr']:.2e}\")\n            \n            self.scheduler.step()\n            \n            # Early stopping\n            if avg_loss < 0.1:\n                print(f\"\\n‚úÖ Excellent loss achieved! Stopping early.\\n\")\n                break\n        \n        print(f\"\\n‚úÖ Training complete! Best loss: {self.best_loss:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.165741Z","iopub.execute_input":"2025-11-26T07:44:51.165922Z","iopub.status.idle":"2025-11-26T07:44:51.182462Z","shell.execute_reply.started":"2025-11-26T07:44:51.165908Z","shell.execute_reply":"2025-11-26T07:44:51.181656Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Cell 13: Evaluation\n# ============================================\n# EVALUATION\n# ============================================\n\ndef evaluate_matching(generated, test_set):\n    unique_gen = set(generated)\n    matches = unique_gen & test_set\n    \n    # Additional metrics\n    gen_lengths = [len(p) for p in generated]\n    test_lengths = [len(p) for p in test_set]\n    \n    results = {\n        'total_generated': len(generated),\n        'unique_generated': len(unique_gen),\n        'uniqueness_rate': len(unique_gen) / len(generated) * 100 if generated else 0,\n        'test_set_size': len(test_set),\n        'matches': len(matches),\n        'match_rate': len(matches) / len(test_set) * 100 if test_set else 0,\n        'matched_passwords': list(matches)[:50],\n        'avg_gen_length': np.mean(gen_lengths) if gen_lengths else 0,\n        'avg_test_length': np.mean(test_lengths) if test_lengths else 0,\n        'sample_generated': generated[:20]\n    }\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.183174Z","iopub.execute_input":"2025-11-26T07:44:51.183503Z","iopub.status.idle":"2025-11-26T07:44:51.198606Z","shell.execute_reply.started":"2025-11-26T07:44:51.183480Z","shell.execute_reply":"2025-11-26T07:44:51.198072Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Cell 14: Save Functions\n# ============================================\n# SAVE GENERATED PASSWORDS TO FILE\n# ============================================\n\ndef save_passwords_to_file(passwords, output_dir, filename=\"generated_passwords.txt\"):\n    \"\"\"Save generated passwords to a text file\"\"\"\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    output_path = Path(output_dir) / filename\n    \n    with open(output_path, 'w', encoding='utf-8') as f:\n        for pwd in passwords:\n            f.write(f\"{pwd}\\n\")\n    \n    return str(output_path)\n\n\ndef save_detailed_results(passwords, test_passwords, output_dir):\n    \"\"\"Save detailed analysis with multiple files\"\"\"\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    \n    # 1. Save all generated passwords\n    all_pwd_path = save_passwords_to_file(passwords, output_dir, \"all_generated_passwords.txt\")\n    print(f\"‚úì All passwords saved: {all_pwd_path}\")\n    \n    # 2. Save unique passwords only\n    unique_passwords = list(set(passwords))\n    unique_pwd_path = save_passwords_to_file(unique_passwords, output_dir, \"unique_generated_passwords.txt\")\n    print(f\"‚úì Unique passwords saved: {unique_pwd_path}\")\n    \n    # 3. Save matched passwords\n    matches = set(passwords) & test_passwords\n    if matches:\n        matched_pwd_path = save_passwords_to_file(sorted(matches), output_dir, \"matched_passwords.txt\")\n        print(f\"‚úì Matched passwords saved: {matched_pwd_path}\")\n    \n    # 4. Save passwords by length\n    length_groups = {}\n    for pwd in unique_passwords:\n        length = len(pwd)\n        if length not in length_groups:\n            length_groups[length] = []\n        length_groups[length].append(pwd)\n    \n    length_stats_path = Path(output_dir) / \"passwords_by_length.txt\"\n    with open(length_stats_path, 'w', encoding='utf-8') as f:\n        f.write(\"PASSWORD LENGTH DISTRIBUTION\\n\")\n        f.write(\"=\"*60 + \"\\n\\n\")\n        \n        for length in sorted(length_groups.keys()):\n            f.write(f\"\\nLength {length}: {len(length_groups[length])} passwords\\n\")\n            f.write(\"-\"*60 + \"\\n\")\n            for pwd in sorted(length_groups[length])[:20]:\n                f.write(f\"{pwd}\\n\")\n            if len(length_groups[length]) > 20:\n                f.write(f\"... and {len(length_groups[length]) - 20} more\\n\")\n    \n    print(f\"‚úì Length analysis saved: {length_stats_path}\")\n    \n    # 5. Save statistics summary\n    stats_path = Path(output_dir) / \"generation_statistics.txt\"\n    with open(stats_path, 'w', encoding='utf-8') as f:\n        f.write(\"PASSWORD GENERATION STATISTICS\\n\")\n        f.write(\"=\"*60 + \"\\n\\n\")\n        \n        f.write(f\"Total Generated:       {len(passwords):,}\\n\")\n        f.write(f\"Unique Passwords:      {len(unique_passwords):,}\\n\")\n        f.write(f\"Uniqueness Rate:       {len(unique_passwords)/len(passwords)*100:.2f}%\\n\\n\")\n        \n        f.write(f\"Test Set Size:         {len(test_passwords):,}\\n\")\n        f.write(f\"Matched Passwords:     {len(matches):,}\\n\")\n        f.write(f\"Match Rate:            {len(matches)/len(test_passwords)*100:.2f}%\\n\\n\")\n        \n        # Length statistics\n        lengths = [len(p) for p in unique_passwords]\n        f.write(f\"Average Length:        {np.mean(lengths):.2f}\\n\")\n        f.write(f\"Min Length:            {min(lengths)}\\n\")\n        f.write(f\"Max Length:            {max(lengths)}\\n\")\n        f.write(f\"Median Length:         {np.median(lengths):.0f}\\n\\n\")\n        \n        # Character statistics\n        f.write(\"CHARACTER USAGE:\\n\")\n        all_chars = ''.join(unique_passwords)\n        char_counter = Counter(all_chars)\n        f.write(f\"Total Characters:      {len(all_chars):,}\\n\")\n        f.write(f\"Unique Characters:     {len(char_counter)}\\n\\n\")\n        \n        f.write(\"Top 20 Most Common Characters:\\n\")\n        for char, count in char_counter.most_common(20):\n            f.write(f\"  '{char}': {count:,} ({count/len(all_chars)*100:.2f}%)\\n\")\n    \n    print(f\"‚úì Statistics saved: {stats_path}\")\n    \n    return {\n        'all_passwords': all_pwd_path,\n        'unique_passwords': unique_pwd_path,\n        'matched_passwords': matched_pwd_path if matches else None,\n        'length_analysis': str(length_stats_path),\n        'statistics': str(stats_path)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.199286Z","iopub.execute_input":"2025-11-26T07:44:51.199510Z","iopub.status.idle":"2025-11-26T07:44:51.214693Z","shell.execute_reply.started":"2025-11-26T07:44:51.199488Z","shell.execute_reply":"2025-11-26T07:44:51.214159Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Cell 15: Main Training Functions (Part 1)\n# ============================================\n# MAIN TRAINING FUNCTIONS\n# ============================================\n\ndef train_single_dataset(dataset_name, num_passwords_to_generate=500000):\n    \"\"\"Train model on a single dataset with GPU optimization\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"ENHANCED PGTCN - Training on {dataset_name.upper()}\")\n    print(f\"Target: Generate {num_passwords_to_generate:,} passwords\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Check GPU status\n    if GPU_AVAILABLE:\n        monitor_gpu_memory()\n    \n    # Load dataset\n    filtered = load_dataset(dataset_name)\n    if filtered is None:\n        print(f\"‚ùå Skipping {dataset_name} due to loading error\\n\")\n        return None\n    \n    # Build vocabulary\n    stoi, itos = build_vocabulary(filtered)\n    Config.VOCAB_SIZE = len(stoi)\n    print(f\"‚úì Vocabulary size: {Config.VOCAB_SIZE}\\n\")\n    \n    # Split train/test (80/20)\n    random.shuffle(filtered)\n    split_idx = int(0.8 * len(filtered))\n    train_passwords = filtered[:split_idx]\n    test_passwords = set(filtered[split_idx:])\n    \n    print(f\"‚úì Train set: {len(train_passwords):,}\")\n    print(f\"‚úì Test set: {len(test_passwords):,}\\n\")\n    \n    # Dataset with augmentation\n    train_dataset = PasswordDataset(train_passwords, stoi, Config.SEQ_LEN, augment=True)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=Config.BATCH_SIZE,\n        shuffle=True,\n        num_workers=Config.NUM_WORKERS,\n        pin_memory=Config.PIN_MEMORY,\n        collate_fn=collate_fn,\n        drop_last=True\n    )\n    \n    # Build model\n    model = PGTCN(\n        vocab_size=Config.VOCAB_SIZE,\n        embedding_dim=Config.EMBEDDING_DIM,\n        num_channels=Config.TCN_CHANNELS,\n        kernel_size=Config.KERNEL_SIZE,\n        dropout=Config.DROPOUT\n    ).to(Config.DEVICE)\n    \n    num_params = sum(p.numel() for p in model.parameters())\n    print(f\"‚úì Model parameters: {num_params:,}\")\n    print(f\"‚úì Model on device: {next(model.parameters()).device}\\n\")\n    \n    # Show GPU memory after model loading\n    if GPU_AVAILABLE:\n        monitor_gpu_memory()\n    \n    # Print configuration\n    Config.print_config()\n    \n    # Train\n    trainer = PGTCNTrainer(model, Config, stoi, itos)\n    trainer.train(train_loader)\n    \n    # Clear GPU cache before generation\n    if GPU_AVAILABLE:\n        clear_gpu_cache()\n    \n    # Load best model\n    best_model_path = f\"{Config.MODEL_DIR}/pgtcn_best.pt\"\n    model.load_state_dict(torch.load(best_model_path, map_location=Config.DEVICE))\n    \n    # Save final model\n    save_dataset_model(model, stoi, itos, dataset_name)\n    \n    # Generate passwords\n    print(\"=\"*80)\n    print(f\"GENERATING {num_passwords_to_generate:,} DIVERSE PASSWORDS\")\n    print(\"=\"*80 + \"\\n\")\n    \n    generator = DiversePasswordGenerator(model, stoi, itos, Config.DEVICE)\n    generated = generator.generate_diverse_batch(num_passwords=num_passwords_to_generate)\n    \n    print(f\"\\n‚úì Successfully generated {len(generated):,} passwords\\n\")\n    \n    # Show final GPU memory usage\n    if GPU_AVAILABLE:\n        monitor_gpu_memory()\n    \n    # Evaluate\n    print(\"=\"*80)\n    print(\"EVALUATION RESULTS\")\n    print(\"=\"*80 + \"\\n\")\n    \n    results = evaluate_matching(generated, test_passwords)\n    \n    print(f\"Generated Passwords:  {results['total_generated']:,}\")\n    print(f\"Unique Generated:     {results['unique_generated']:,} ({results['uniqueness_rate']:.1f}%)\")\n    print(f\"Test Set Size:        {results['test_set_size']:,}\")\n    print(f\"Exact Matches:        {results['matches']:,}\")\n    print(f\"üéØ MATCH RATE:        {results['match_rate']:.2f}%\")\n    print(f\"Avg Gen Length:       {results['avg_gen_length']:.1f}\")\n    print(f\"Avg Test Length:      {results['avg_test_length']:.1f}\")\n    print(f\"{'='*80}\\n\")\n    \n    # Save results\n    print(\"=\"*80)\n    print(\"SAVING RESULTS TO FILES\")\n    print(\"=\"*80 + \"\\n\")\n    \n    json_path = f\"{Config.MODEL_DIR}/results.json\"\n    with open(json_path, 'w') as f:\n        json.dump(results, f, indent=2)\n    print(f\"‚úì JSON results saved: {json_path}\\n\")\n    \n    saved_files = save_detailed_results(generated, test_passwords, Config.MODEL_DIR)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"FILES SAVED:\")\n    print(\"=\"*80)\n    for file_type, file_path in saved_files.items():\n        if file_path:\n            print(f\"  ‚Ä¢ {file_type}: {file_path}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"‚úÖ {dataset_name.upper()} Complete!\")\n    print(f\"   Generated: {len(generated):,} passwords\")\n    print(f\"   Match Rate: {results['match_rate']:.2f}%\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Clean up GPU memory\n    if GPU_AVAILABLE:\n        clear_gpu_cache()\n    \n    return {\n        'dataset': dataset_name,\n        'results': results,\n        'files': saved_files,\n        'model_dir': Config.MODEL_DIR,\n        'num_generated': len(generated)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.215436Z","iopub.execute_input":"2025-11-26T07:44:51.215760Z","iopub.status.idle":"2025-11-26T07:44:51.239524Z","shell.execute_reply.started":"2025-11-26T07:44:51.215739Z","shell.execute_reply":"2025-11-26T07:44:51.238859Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Cell 16: Main Training Functions (Part 2)\ndef test_on_dataset(model, stoi, itos, test_dataset_name, source_dataset_name, num_passwords=100000):\n    \"\"\"Test a trained model on a different dataset\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"CROSS-DATASET TESTING - Generating {num_passwords:,} passwords\")\n    print(f\"Model: {source_dataset_name.upper()} ‚Üí Test: {test_dataset_name.upper()}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Load test dataset\n    test_passwords = load_dataset(test_dataset_name)\n    if test_passwords is None:\n        return None\n    \n    test_set = set(test_passwords)\n    \n    # Generate passwords\n    generator = DiversePasswordGenerator(model, stoi, itos, Config.DEVICE)\n    generated = generator.generate_diverse_batch(num_passwords=num_passwords)\n    \n    # Evaluate\n    results = evaluate_matching(generated, test_set)\n    \n    print(f\"Generated Passwords:  {results['total_generated']:,}\")\n    print(f\"Unique Generated:     {results['unique_generated']:,} ({results['uniqueness_rate']:.1f}%)\")\n    print(f\"Test Set Size:        {results['test_set_size']:,}\")\n    print(f\"Exact Matches:        {results['matches']:,}\")\n    print(f\"üéØ MATCH RATE:        {results['match_rate']:.2f}%\")\n    print(f\"{'='*80}\\n\")\n    \n    # Save cross-testing results\n    output_dir = f\"{Config.DATASETS[source_dataset_name]['model_dir']}/cross_test_{test_dataset_name}/\"\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    \n    json_path = f\"{output_dir}/results.json\"\n    with open(json_path, 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    saved_files = save_detailed_results(generated, test_set, output_dir)\n    \n    return {\n        'source_dataset': source_dataset_name,\n        'test_dataset': test_dataset_name,\n        'results': results,\n        'files': saved_files\n    }\n\n\ndef evaluate_dataset_with_checkpoint(dataset_name, num_passwords_to_generate=500000, use_beam_search=True):\n    \"\"\"Use an existing checkpoint + vocab files to generate and evaluate passwords.\"\"\"\n    Config.set_dataset(dataset_name)\n    model_dir = Path(Config.DATASETS[dataset_name]['model_dir'])\n    model_path = model_dir / \"pgtcn_final.pt\"\n    stoi_path = model_dir / \"stoi.json\"\n    itos_path = model_dir / \"itos.json\"\n\n    if not (model_path.exists() and stoi_path.exists() and itos_path.exists()):\n        print(f\"‚ö†Ô∏è  Checkpoint or vocab files missing for {dataset_name}. Training will be required.\")\n        return None\n\n    filtered = load_dataset(dataset_name)\n    if filtered is None:\n        return None\n\n    random.shuffle(filtered)\n    split_idx = int(0.8 * len(filtered))\n    test_passwords = set(filtered[split_idx:])\n\n    checkpoint_result = generate_passwords_from_checkpoint(\n        model_path=model_path,\n        stoi_path=stoi_path,\n        itos_path=itos_path,\n        num_passwords=num_passwords_to_generate,\n        use_beam_search=use_beam_search,\n        save_dir=model_dir,\n        filename=\"generated_from_checkpoint.txt\"\n    )\n\n    generated = checkpoint_result['passwords']\n    print(f\"\\n‚úì Successfully generated {len(generated):,} passwords from checkpoint\\n\")\n\n    results = evaluate_matching(generated, test_passwords)\n\n    print(\"=\"*80)\n    print(\"EVALUATION RESULTS (CHECKPOINT)\")\n    print(\"=\"*80 + \"\\n\")\n    print(f\"Generated Passwords:  {results['total_generated']:,}\")\n    print(f\"Unique Generated:     {results['unique_generated']:,} ({results['uniqueness_rate']:.1f}%)\")\n    print(f\"Test Set Size:        {results['test_set_size']:,}\")\n    print(f\"Exact Matches:        {results['matches']:,}\")\n    print(f\"üéØ MATCH RATE:        {results['match_rate']:.2f}%\")\n    print(f\"Avg Gen Length:       {results['avg_gen_length']:.1f}\")\n    print(f\"Avg Test Length:      {results['avg_test_length']:.1f}\")\n    print(f\"{'='*80}\\n\")\n\n    json_path = f\"{model_dir}/results_checkpoint.json\"\n    with open(json_path, 'w') as f:\n        json.dump(results, f, indent=2)\n    print(f\"‚úì Checkpoint evaluation saved: {json_path}\\n\")\n\n    saved_files = save_detailed_results(generated, test_passwords, model_dir)\n\n    return {\n        'dataset': dataset_name,\n        'results': results,\n        'files': saved_files,\n        'model_dir': str(model_dir),\n        'num_generated': len(generated),\n        'checkpoint_used': True\n    }\n\n\ndef main(num_passwords_per_dataset=100000, use_existing_checkpoints=True):\n    \"\"\"Main function to train and generate passwords on multiple datasets\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ENHANCED PGTCN - Multi-Dataset Training & Password Generation\")\n    print(f\"GPU-Optimized for CUDA 12.8 with RTX 4500 Ada\")\n    print(f\"Generating {num_passwords_per_dataset:,} passwords per dataset\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Show available datasets\n    available = Config.get_available_datasets()\n    print(\"Available datasets:\")\n    for i, dataset in enumerate(available, 1):\n        desc = Config.DATASETS[dataset]['description']\n        print(f\"  {i}. {dataset} - {desc}\")\n    print()\n    \n    print(\"=\"*80)\n    print(f\"TRAINING ON ALL DATASETS - {num_passwords_per_dataset:,} passwords each\")\n    print(\"=\"*80 + \"\\n\")\n    \n    all_results = []\n    total_passwords_generated = 0\n    \n    for dataset_name in available:\n        try:\n            print(f\"\\n{'#'*80}\")\n            print(f\"# PROCESSING: {dataset_name.upper()}\")\n            print(f\"{'#'*80}\\n\")\n\n            model_dir = Path(Config.DATASETS[dataset_name]['model_dir'])\n            checkpoint_path = model_dir / \"pgtcn_final.pt\"\n            stoi_path = model_dir / \"stoi.json\"\n            itos_path = model_dir / \"itos.json\"\n            checkpoint_exists = checkpoint_path.exists() and stoi_path.exists() and itos_path.exists()\n\n            if use_existing_checkpoints and checkpoint_exists:\n                print(f\"üîÅ Existing checkpoint detected for {dataset_name}. Skipping training.\")\n                result = evaluate_dataset_with_checkpoint(dataset_name, num_passwords_to_generate=num_passwords_per_dataset)\n            else:\n                result = train_single_dataset(dataset_name, num_passwords_per_dataset)\n            \n            if result:\n                all_results.append(result)\n                total_passwords_generated += result['num_generated']\n                \n        except Exception as e:\n            print(f\"\\n{'='*80}\")\n            print(f\"‚ùå Error processing {dataset_name}: {str(e)}\")\n            print(f\"{'='*80}\\n\")\n            import traceback\n            traceback.print_exc()\n            continue\n    \n    # Final summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING & GENERATION SUMMARY\")\n    print(\"=\"*80 + \"\\n\")\n    \n    if not all_results:\n        print(\"‚ùå No datasets were successfully processed!\")\n        return None\n    \n    print(f\"{'Dataset':<15} {'Generated':>12} {'Unique':>12} {'Match Rate':>12} {'Matches':>10}\")\n    print(\"=\"*80)\n    \n    for result in all_results:\n        dataset = result['dataset']\n        num_gen = result['results']['total_generated']\n        num_unique = result['results']['unique_generated']\n        match_rate = result['results']['match_rate']\n        matches = result['results']['matches']\n        \n        print(f\"{dataset.upper():<15} \"\n              f\"{num_gen:>11,} \"\n              f\"{num_unique:>11,} \"\n              f\"{match_rate:>11.2f}% \"\n              f\"{matches:>10,}\")\n    \n    print(\"=\"*80)\n    print(f\"\\nTOTAL PASSWORDS GENERATED: {total_passwords_generated:,}\")\n    print(f\"DATASETS PROCESSED: {len(all_results)}/{len(available)}\")\n    print(\"\\n\" + \"=\"*80)\n    \n    # Save overall summary\n    summary_path = \"./pgtcn_modelMulti/training_summary.json\"\n    Path(\"./pgtcn_modelMulti/\").mkdir(parents=True, exist_ok=True)\n    \n    summary_data = {\n        'total_passwords_generated': total_passwords_generated,\n        'datasets_processed': len(all_results),\n        'datasets_failed': len(available) - len(all_results),\n        'passwords_per_dataset': num_passwords_per_dataset,\n        'results': [\n            {\n                'dataset': r['dataset'],\n                'generated': r['results']['total_generated'],\n                'unique': r['results']['unique_generated'],\n                'match_rate': r['results']['match_rate'],\n                'matches': r['results']['matches'],\n                'model_dir': r['model_dir'],\n                'checkpoint_used': r.get('checkpoint_used', False)\n            }\n            for r in all_results\n        ]\n    }\n    \n    with open(summary_path, 'w') as f:\n        json.dump(summary_data, f, indent=2)\n    \n    print(f\"\\n‚úì Overall summary saved: {summary_path}\\n\")\n    print(\"=\"*80)\n    print(\"‚úÖ ALL DATASETS PROCESSING COMPLETE!\")\n    print(\"=\"*80 + \"\\n\")\n    \n    return all_results\n\n\ndef train_specific_dataset(dataset_name, num_passwords=500000):\n    \"\"\"Quick function to train on a specific dataset\"\"\"\n    if dataset_name not in Config.get_available_datasets():\n        print(f\"‚ùå Dataset '{dataset_name}' not found!\")\n        print(f\"Available: {Config.get_available_datasets()}\")\n        return None\n    \n    return train_single_dataset(dataset_name, num_passwords)\n\n\ndef train_specific_datasets(dataset_list, num_passwords=500000):\n    \"\"\"Train on multiple specific datasets\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"TRAINING ON SPECIFIC DATASETS - {num_passwords:,} passwords each\")\n    print(f\"Datasets: {', '.join(dataset_list)}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    results = []\n    total_passwords = 0\n    \n    for dataset in dataset_list:\n        if dataset not in Config.get_available_datasets():\n            print(f\"‚ö†Ô∏è  Skipping unknown dataset: {dataset}\")\n            continue\n        \n        result = train_specific_dataset(dataset, num_passwords)\n        if result:\n            results.append(result)\n            total_passwords += result['num_generated']\n    \n    # Summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"SPECIFIC DATASETS SUMMARY\")\n    print(\"=\"*80 + \"\\n\")\n    \n    for result in results:\n        dataset = result['dataset']\n        match_rate = result['results']['match_rate']\n        num_gen = result['num_generated']\n        print(f\"{dataset.upper():15} ‚Üí Generated: {num_gen:,} | Match Rate: {match_rate:6.2f}%\")\n    \n    print(f\"\\nTotal passwords generated: {total_passwords:,}\")\n    print(\"=\"*80 + \"\\n\")\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.240342Z","iopub.execute_input":"2025-11-26T07:44:51.240610Z","iopub.status.idle":"2025-11-26T07:44:51.263442Z","shell.execute_reply.started":"2025-11-26T07:44:51.240594Z","shell.execute_reply":"2025-11-26T07:44:51.262892Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Cell 19: Checkpoint Generation Launcher\n# ============================================\n# QUICK CALLER TO GENERATE PASSWORDS FROM A CHECKPOINT\n# ============================================\n\n\ndef evaluate_dataset_with_checkpoint(myspace,\n                              num_passwords=100000,\n                              use_beam_search=True,\n                              filename=\"generated_from_checkpoint.txt\"):\n    \"\"\"Convenience wrapper to load dataset checkpoint + vocab and generate passwords.\"\"\"\n    Config.set_dataset(dataset_name)\n    model_dir = Path(Config.DATASETS[dataset_name]['model_dir'])\n\n    result = generate_passwords_from_checkpoint(\n        model_path=model_dir / \"pgtcn_final.pt\",\n        stoi_path=model_dir / \"stoi.json\",\n        itos_path=model_dir / \"itos.json\",\n        num_passwords=num_passwords,\n        use_beam_search=use_beam_search,\n        save_dir=model_dir,\n        filename=filename\n    )\n\n    print(f\"\\nFinished checkpoint generation for {dataset_name} ‚Üí Saved: {result['saved_path']}\")\n    return result\n\n\n# Example invocation (uncomment and adjust dataset_name as needed)\n# checkpoint_result = run_checkpoint_generation(\n#     dataset_name='myspace',\n#     num_passwords=300000,\n#     use_beam_search=True,\n#     filename='myspace_checkpoint_generation.txt'\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.264017Z","iopub.execute_input":"2025-11-26T07:44:51.264263Z","iopub.status.idle":"2025-11-26T07:44:51.279836Z","shell.execute_reply.started":"2025-11-26T07:44:51.264243Z","shell.execute_reply":"2025-11-26T07:44:51.279144Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Cell 17: Run Training\n# ============================================\n# RUN TRAINING - Choose your option\n# ============================================\n\n# OPTION 1: Train ALL datasets with 500K passwords each\nresults = main(num_passwords_per_dataset=100000)\n\n# OPTION 2: Train SPECIFIC datasets\n# datasets_to_train = ['myspace', 'rockyou']\n# results = train_specific_datasets(datasets_to_train, num_passwords=500000)\n\n# OPTION 3: Train SINGLE dataset\n# result = train_specific_dataset('myspace', num_passwords=500000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T07:44:51.280419Z","iopub.execute_input":"2025-11-26T07:44:51.281263Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nENHANCED PGTCN - Multi-Dataset Training & Password Generation\nGPU-Optimized for CUDA 12.8 with RTX 4500 Ada\nGenerating 100,000 passwords per dataset\n================================================================================\n\nAvailable datasets:\n  1. myspace - MySpace password leak\n\n================================================================================\nTRAINING ON ALL DATASETS - 100,000 passwords each\n================================================================================\n\n\n################################################################################\n# PROCESSING: MYSPACE\n################################################################################\n\n\n================================================================================\nENHANCED PGTCN - Training on MYSPACE\nTarget: Generate 100,000 passwords\n================================================================================\n\n\n============================================================\nGPU MEMORY STATUS\n============================================================\nAllocated:    0.00 GB (  0.0%)\nReserved:     0.00 GB (  0.0%)\nFree:        15.83 GB (100.0%)\nTotal:       15.83 GB\n============================================================\n\n\n================================================================================\nLoading MySpace password leak\n================================================================================\n\n‚úì Loaded 37,126 passwords\n‚úì Filtered: 36,829 passwords (4-16 chars, ASCII only)\n\n‚úì Vocabulary size: 95\n\n‚úì Train set: 29,463\n‚úì Test set: 7,366\n\n‚úì Model parameters: 9,541,727\n‚úì Model on device: cuda:0\n\n\n============================================================\nGPU MEMORY STATUS\n============================================================\nAllocated:    0.04 GB (  0.2%)\nReserved:     0.04 GB (  0.3%)\nFree:        15.78 GB ( 99.7%)\nTotal:       15.83 GB\n============================================================\n\n\n============================================================\nCONFIGURATION\n============================================================\nEmbedding Dim:     512\nTCN Channels:      [512, 512, 512, 512, 512, 512]\nKernel Size:       3\nDropout:           0.1\nBatch Size:        64\nLearning Rate:     0.0003\nEpochs:            20\nDevice:            cuda\nUse AMP:           True\nUse Focal Loss:    True\n============================================================\n\n\n================================================================================\nTraining ENHANCED PGTCN for 20 epochs\n================================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:10<00:00, 44.01it/s, loss=0.3182]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 | Loss: 0.3391 | Best: 0.3391 | LR: 3.00e-04\n","output_type":"stream"},{"name":"stderr","text":"Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 397/460 [00:07<00:01, 53.05it/s, loss=0.2950]","output_type":"stream"}],"execution_count":null}]}